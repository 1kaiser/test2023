{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2023/blob/main/GoogleEarthEngine_AnySatellite_Data_Download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy4HceOol_h1"
      },
      "source": [
        "# Install the library\n",
        "!pip -q install FireHR==0.1.2 pyhdf==0.10.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKyI9oFmmvpd"
      },
      "source": [
        "# Authenticate to use Google Earth Engine API\n",
        "import ee\n",
        "ee.Authenticate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "VkDzjn8au_6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S2**"
      ],
      "metadata": {
        "id": "qaYr__CZrM7E"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-rR0f-GmELJ"
      },
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"COPERNICUS/S2\"]  # Product id in google earth engine\n",
        "bands       = ['B4', 'B3', 'B2'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2021-03-01', \n",
        "             time_end     = '2021-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts(R, products, bands, path_save, show_progress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBTGABv5sm0d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from banet.data import open_tif\n",
        "\n",
        "brightness = 3\n",
        "im = np.concatenate([open_tif(f'data/download.{b}.tif').read() for b in bands])\n",
        "im = im.transpose(1,2,0).astype(np.float32)/10000\n",
        "plt.imshow(brightness*im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**GEDI**"
      ],
      "metadata": {
        "id": "Namgq3XVrIgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"LARSE/GEDI/GEDI02_A_002_MONTHLY\"]  # Product id in google earth engine\n",
        "bands       = ['rh98', 'rh38', 'rh10'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-03-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "KOre-TKTrGsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EO1/HYPERION**"
      ],
      "metadata": {
        "id": "EP8-jdvMdPk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"EO1/HYPERION\"]  # Product id in google earth engine\n",
        "bands       = ['B104', 'B140', 'B221'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2015-03-01', \n",
        "             time_end     = '2016-07-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "-dgTB5aNdVMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR**"
      ],
      "metadata": {
        "id": "C1hnMrqQklVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR\"]  # Product id in google earth engine\n",
        "bands       = ['HH', 'HV'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-03-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "ZTOhM6KakmxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**bare code**"
      ],
      "metadata": {
        "id": "_bmbtllFy7KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "     \n",
        "\n",
        "#export\n",
        "class RegionST(Region):\n",
        "    \"Defines a region in space and time with a name, a bounding box and the pixel size.\"\n",
        "    def __init__(self, name:str, bbox:list, pixel_size:float=None, scale_meters:int=None,\n",
        "                 time_start:str=None, time_end:str=None, time_freq:str='D', time_margin:int=0,\n",
        "                 shape:tuple=None, epsg=4326):\n",
        "        if scale_meters is not None and pixel_size is not None: \n",
        "            raise Exception('Either pixel_size or scale_meters must be set to None.')\n",
        "        self.name = name\n",
        "        self.bbox = rasterio.coords.BoundingBox(*bbox) # left, bottom, right, top\n",
        "        if pixel_size is not None:\n",
        "            self.pixel_size = pixel_size\n",
        "        else:\n",
        "            self.pixel_size = scale_meters/111000\n",
        "        self.epsg         = epsg\n",
        "        self.scale_meters = scale_meters\n",
        "        self._shape       = shape\n",
        "        self.time_start   = pd.Timestamp(str(time_start))\n",
        "        self.time_end     = pd.Timestamp(str(time_end))\n",
        "        self.time_margin  = time_margin\n",
        "        self.time_freq    = time_freq\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        \"Shape of the region (height, width)\"\n",
        "        if self._shape is None:\n",
        "            return (self.height, self.width)\n",
        "        else: return self._shape\n",
        "        \n",
        "    @property\n",
        "    def times(self):\n",
        "        \"Property that computes the date_range for the region.\"\n",
        "        tstart = self.time_start - pd.Timedelta(days=self.time_margin)\n",
        "        tend = self.time_end + pd.Timedelta(days=self.time_margin)\n",
        "        return pd.date_range(tstart, tend, freq=self.time_freq)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file, time_start=None, time_end=None):\n",
        "        \"Loads region information from json file\"\n",
        "        with open(file, 'r') as f:\n",
        "            args = json.load(f)\n",
        "        if time_start is None:\n",
        "            time_start = args['time_start']\n",
        "        if time_end is None:\n",
        "            time_end = args['time_end']\n",
        "        return cls(args['name'], args['bbox'], args['pixel_size'],\n",
        "                   time_start=time_start, time_end=time_end)\n",
        "    \n",
        "def extract_region(df_row, cls=Region):\n",
        "    \"Create Region object from a row of the metadata dataframe.\"\n",
        "    if issubclass(cls, RegionST):\n",
        "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size, \n",
        "                   df_row.time_start, df_row.time_end)\n",
        "    elif issubclass(cls, Region):\n",
        "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size)\n",
        "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')\n",
        "     \n",
        "\n",
        "#export\n",
        "def coords2bbox(lon, lat, pixel_size): \n",
        "    return [lon.min(), lat.min(), lon.max()+pixel_size, lat.max()+pixel_size]\n",
        "\n",
        "def split_region(region:RegionST, size:int, cls=Region):\n",
        "    lon, lat = region.coords()\n",
        "    Nlon = (len(lon)//size)*size\n",
        "    Nlat = (len(lat)//size)*size\n",
        "    lons = [*lon[:Nlon].reshape(-1, size), lon[Nlon:][None]]\n",
        "    lats = [*lat[:Nlat].reshape(-1, size), lat[Nlat:][None]]\n",
        "    if len(lats[-1].reshape(-1)) == 0 and len(lons[-1].reshape(-1)) == 0:\n",
        "        lons = lons[:-1]\n",
        "        lats = lats[:-1]\n",
        "    #lons = lon.reshape(-1, size)\n",
        "    #lats = lat.reshape(-1, size)\n",
        "    if issubclass(cls, RegionST):\n",
        "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), \n",
        "                    pixel_size=region.pixel_size, time_start=region.time_start,\n",
        "                    time_end=region.time_end, time_freq=region.time_freq,\n",
        "                    time_margin=region.time_margin) for ilon in lons for ilat in lats]\n",
        "    elif issubclass(cls, Region):\n",
        "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), pixel_size=region.pixel_size) \n",
        "            for ilon in lons for ilat in lats]\n",
        "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')\n",
        "        \n",
        "    return \n",
        "            \n",
        "def merge_tifs(files:list, fname:str, delete=False):\n",
        "    data, tfm = merge([open_tif(str(f)) for f in files])\n",
        "    data = data.squeeze()\n",
        "    fname = Path(files[0]).parent/fname\n",
        "    profile = open_tif(str(files[0])).profile\n",
        "    with rasterio.Env():\n",
        "        height, width = data.shape\n",
        "        profile.update(width=width, height=height, transform=tfm, compress='lzw')\n",
        "        with rasterio.open(str(fname), 'w', **profile) as dst:\n",
        "            dst.write(data, 1)\n",
        "    if delete:\n",
        "        for f in files: os.remove(f)\n",
        "     \n",
        "\n",
        "#export\n",
        "def filter_region(image_collection:ee.ImageCollection, region:RegionST, times:tuple, bands=None):\n",
        "    image_collection = image_collection.filterDate(times[0], times[1])\n",
        "    geometry = ee.Geometry.Rectangle(region.bbox)\n",
        "    image_collection = image_collection.filterBounds(geometry)\n",
        "    if bands is not None:\n",
        "        image_collection = image_collection.select(bands)\n",
        "    return image_collection\n",
        "\n",
        "def filter_cloudy(image_collection:ee.ImageCollection, max_cloud_fraction=0.2):\n",
        "    return image_collection.filterMetadata(\n",
        "        'CLOUDY_PIXEL_PERCENTAGE', 'not_greater_than', max_cloud_fraction)\n",
        "\n",
        "def n_least_cloudy(image_collection:ee.ImageCollection, n=5):\n",
        "    image_collection = image_collection.sort(prop='CLOUDY_PIXEL_PERCENTAGE')\n",
        "    image_collection = image_collection.toList(image_collection.size())\n",
        "    colsize = image_collection.size().getInfo()\n",
        "    if colsize < n: \n",
        "        warnings.warn(f'Total number of images in the collection {colsize} less than n={n}. Setting n={colsize}')\n",
        "        n = colsize\n",
        "    image_collection = ee.ImageCollection([ee.Image(image_collection.get(i)) for i in range(n)])\n",
        "    return image_collection\n",
        "\n",
        "def download_topography_data(R:RegionST, path_save=Path('.'), scale=None, \n",
        "                             download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    image = ee.Image('srtm90_v4')\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size)\n",
        "    if not (path_save/'srtm90_v4.elevation.tif').is_file():\n",
        "        files = []\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" +\n",
        "              f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "            url = image.getDownloadUrl(\n",
        "                {'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "            r = requests.get(url)\n",
        "            with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                f.write(r.content)\n",
        "            with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                f.extractall(str(path_save))\n",
        "                os.rename(str(path_save/'srtm90_v4.elevation.tif'),\n",
        "                          str(path_save/f'srtm90_v4.elevation_{j}.tif'))\n",
        "                files.append(str(path_save/f'srtm90_v4.elevation_{j}.tif'))\n",
        "            os.remove(str(path_save/'data.zip'))\n",
        "        merge_tifs(files, 'srtm90_v4.elevation.tif', delete=True)\n",
        "\n",
        "def download_data(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" +\n",
        "                       f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                for i in range(1, len(products)):\n",
        "                    imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                            zip_error = True\n",
        "                            for i in range(10): # Try 10 times\n",
        "                                if zip_error:\n",
        "                                    try:\n",
        "                                        url = image.getDownloadURL(\n",
        "                                            {'scale': scale, 'crs': 'EPSG:4326', \n",
        "                                             'region': f'{region}'})\n",
        "                                        r = requests.get(url)\n",
        "                                        with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                                            f.write(r.content)\n",
        "                                        with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                                            files = f.namelist()\n",
        "                                            f.extractall(str(path_save))\n",
        "                                        os.remove(str(path_save/'data.zip'))\n",
        "                                        zip_error = False\n",
        "                                    except:\n",
        "                                        zip_error = True\n",
        "                                        os.remove(str(path_save/'data.zip'))\n",
        "                                        time.sleep(10)\n",
        "                            if zip_error: raise Exception(f'Failed to process {url}')\n",
        "                            for f in files:\n",
        "                                f = path_save/f\n",
        "                                os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "            \n",
        "def download_data_ts(R:RegionST, products, bands, path_save, scale=None, \n",
        "                     download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    for j, R in loop:\n",
        "        region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "        # Merge products to single image collection\n",
        "        imCol = ee.ImageCollection(products[0])\n",
        "        for i in range(1, len(products)):\n",
        "            imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
        "        imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "        imCol = ee.ImageCollection(imCol)\n",
        "        colList = imCol.toList(imCol.size())\n",
        "\n",
        "        # Download each image\n",
        "        for i in range(colList.size().getInfo()):\n",
        "            image = ee.Image(colList.get(i))\n",
        "            zip_error = True\n",
        "            for i in range(10): # Try 10 times\n",
        "                if zip_error:\n",
        "                    try:\n",
        "                        url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})\n",
        "                        r = requests.get(url)\n",
        "                        with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                            f.write(r.content)\n",
        "                        with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                            files = f.namelist()\n",
        "                            f.extractall(str(path_save))\n",
        "                        os.remove(str(path_save/'data.zip'))\n",
        "                        zip_error = False\n",
        "                    except:\n",
        "                        zip_error = True\n",
        "                        os.remove(str(path_save/'data.zip'))\n",
        "                        time.sleep(10)\n",
        "            if zip_error: raise Exception(f'Failed to process {url}')\n",
        "            for f in files:\n",
        "                f = path_save/f\n",
        "                os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "                \n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "cgDoL5qty-Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**fun**"
      ],
      "metadata": {
        "id": "gsZow793zR3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR\"]  # Product id in google earth engine\n",
        "bands       = ['HH', 'HV'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-05-30', \n",
        "             time_end     = '2020-06-20')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "#download_data(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "download_data_ts(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "3-giNuSezZxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**test parallel**"
      ],
      "metadata": {
        "id": "5uoTb-cE4WY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "    !curl -L -O --remote-header-name \\\n",
        "      --header \"Authorization: Bearer PNPKuJSlDQavPosdUzw4DyJRP0-P4zNr3ME4ovJPvjyqM3bOxCPB1JbDHczTZrUEb2YfpKfokClVoKOH5yzoDw\" \\\n",
        "      --location {url}\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('time (s):', result)\n",
        "        # output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "file1 = open(\"/content/url.txt\", 'r')\n",
        "###########################################################################################################################\n",
        "download_parallel(file1)"
      ],
      "metadata": {
        "id": "0HYj2GPP4bdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool          \n",
        "cpus = cpu_count()\n",
        "def download_data_ts(R:RegionST, products, bands, path_save, scale=None, \n",
        "                     download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    print(loop)\n",
        "    for j, R in loop:\n",
        "        region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "        # Merge products to single image collection\n",
        "        imCol = ee.ImageCollection(products[0])\n",
        "        for i in range(1, len(products)):\n",
        "            imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
        "        imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "        imCol = ee.ImageCollection(imCol)\n",
        "        colList = imCol.toList(imCol.size())\n",
        "        print(colList.size().getInfo())\n",
        "\n",
        "        # Download each image\n",
        "        results = ThreadPool(cpus - 1).imap_unordered(download_url, colList.size().getInfo())\n",
        "        for result in results:\n",
        "          print('time (s):', result)\n",
        "\n",
        "                \n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "Yr22qb0kBJPl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_url(input, colList):\n",
        "  image = ee.Image(list.get(input))\n",
        "  zip_error = True\n",
        "  for i in range(10): # Try 10 times\n",
        "      if zip_error:\n",
        "          try:\n",
        "              url = image.getDownloadURL(\n",
        "                  {'scale': scale, 'crs': 'EPSG:4326', \n",
        "                    'region': f'{region}'})\n",
        "              r = requests.get(url)\n",
        "              with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                  f.write(r.content)\n",
        "              with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                  files = f.namelist()\n",
        "                  f.extractall(str(path_save))\n",
        "              os.remove(str(path_save/'data.zip'))\n",
        "              zip_error = False\n",
        "          except:\n",
        "              zip_error = True\n",
        "              os.remove(str(path_save/'data.zip'))\n",
        "              time.sleep(10)\n",
        "  if zip_error: raise Exception(f'Failed to process {url}')\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))"
      ],
      "metadata": {
        "id": "Q8WC1o3pCHlh"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}