{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2023/blob/main/GoogleEarthEngine_AnySatellite_Data_Download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy4HceOol_h1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b36cc0-1ff6-4ea6-a15b-1c36080e85bb"
      },
      "source": [
        "# Install the library\n",
        "!pip -q install FireHR==0.1.2 pyhdf==0.10.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/149.1 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m143.4/149.1 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.3/193.3 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m458.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.6/133.6 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.1/439.1 KB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.0/424.0 KB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.2/365.2 KB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.3.3 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for pyhdf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cdsapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook~=5.7.16, but you have notebook 6.5.2 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=6.0.4, but you have tornado 6.2 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.0.3 which is incompatible.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.9 which is incompatible.\n",
            "confection 0.0.3 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKyI9oFmmvpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cfe25d8-12e9-4f5c-d469-4d306109f188"
      },
      "source": [
        "# Authenticate to use Google Earth Engine API\n",
        "import ee\n",
        "ee.Authenticate()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=7TotBWZ_Rj7DIjcQ5a1wVuoRHs0NcCNu4aesqaoTBZo&tc=GcjRDFCI14pIbi_XjQTYVrlMv5KlRGxF_REyynGjYn4&cc=Fu4MPb3WH9j-Ky75RdRNrtuhL3UsY-l-PY5CamoRRxo\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below.\n",
            "Enter verification code: 4/1AWtgzh7cYvl03sSb-4AKBhqz7bUzjNNumcKEbWBdyMWlsHo_3FO6YqVNjuE\n",
            "\n",
            "Successfully saved authorization token.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z87ijnz_4gMy",
        "outputId": "99b213ca-91b5-49e8-8da7-2576e32bf4ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/OUT/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IypPoKGP5LrP",
        "outputId": "967c0fa4-3b82-4202-e883-97c01ccbb621"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/OUT/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**main code**"
      ],
      "metadata": {
        "id": "C1hnMrqQklVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bare code**"
      ],
      "metadata": {
        "id": "_bmbtllFy7KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "     \n",
        "from FireHR.data import *\n",
        "#export\n",
        "class RegionST(Region):\n",
        "    \"Defines a region in space and time with a name, a bounding box and the pixel size.\"\n",
        "    def __init__(self, name:str, bbox:list, pixel_size:float=None, scale_meters:int=None,\n",
        "                 time_start:str=None, time_end:str=None, time_freq:str='D', time_margin:int=0,\n",
        "                 shape:tuple=None, epsg=4326):\n",
        "        if scale_meters is not None and pixel_size is not None: \n",
        "            raise Exception('Either pixel_size or scale_meters must be set to None.')\n",
        "        self.name = name\n",
        "        self.bbox = rasterio.coords.BoundingBox(*bbox) # left, bottom, right, top\n",
        "        if pixel_size is not None:\n",
        "            self.pixel_size = pixel_size\n",
        "        else:\n",
        "            self.pixel_size = scale_meters/111000\n",
        "        self.epsg         = epsg\n",
        "        self.scale_meters = scale_meters\n",
        "        self._shape       = shape\n",
        "        self.time_start   = pd.Timestamp(str(time_start))\n",
        "        self.time_end     = pd.Timestamp(str(time_end))\n",
        "        self.time_margin  = time_margin\n",
        "        self.time_freq    = time_freq\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        \"Shape of the region (height, width)\"\n",
        "        if self._shape is None:\n",
        "            return (self.height, self.width)\n",
        "        else: return self._shape\n",
        "        \n",
        "    @property\n",
        "    def times(self):\n",
        "        \"Property that computes the date_range for the region.\"\n",
        "        tstart = self.time_start - pd.Timedelta(days=self.time_margin)\n",
        "        tend = self.time_end + pd.Timedelta(days=self.time_margin)\n",
        "        return pd.date_range(tstart, tend, freq=self.time_freq)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file, time_start=None, time_end=None):\n",
        "        \"Loads region information from json file\"\n",
        "        with open(file, 'r') as f:\n",
        "            args = json.load(f)\n",
        "        if time_start is None:\n",
        "            time_start = args['time_start']\n",
        "        if time_end is None:\n",
        "            time_end = args['time_end']\n",
        "        return cls(args['name'], args['bbox'], args['pixel_size'],\n",
        "                   time_start=time_start, time_end=time_end)\n",
        "    \n",
        "def extract_region(df_row, cls=Region):\n",
        "    \"Create Region object from a row of the metadata dataframe.\"\n",
        "    if issubclass(cls, RegionST):\n",
        "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size, \n",
        "                   df_row.time_start, df_row.time_end)\n",
        "    elif issubclass(cls, Region):\n",
        "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size)\n",
        "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')\n",
        "     \n",
        "\n",
        "#export\n",
        "def coords2bbox(lon, lat, pixel_size): \n",
        "    return [lon.min(), lat.min(), lon.max()+pixel_size, lat.max()+pixel_size]\n",
        "\n",
        "def split_region(region:RegionST, size:int, cls=Region):\n",
        "    lon, lat = region.coords()\n",
        "    Nlon = (len(lon)//size)*size\n",
        "    Nlat = (len(lat)//size)*size\n",
        "    lons = [*lon[:Nlon].reshape(-1, size), lon[Nlon:][None]]\n",
        "    lats = [*lat[:Nlat].reshape(-1, size), lat[Nlat:][None]]\n",
        "    if len(lats[-1].reshape(-1)) == 0 and len(lons[-1].reshape(-1)) == 0:\n",
        "        lons = lons[:-1]\n",
        "        lats = lats[:-1]\n",
        "    #lons = lon.reshape(-1, size)\n",
        "    #lats = lat.reshape(-1, size)\n",
        "    if issubclass(cls, RegionST):\n",
        "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), \n",
        "                    pixel_size=region.pixel_size, time_start=region.time_start,\n",
        "                    time_end=region.time_end, time_freq=region.time_freq,\n",
        "                    time_margin=region.time_margin) for ilon in lons for ilat in lats]\n",
        "    elif issubclass(cls, Region):\n",
        "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), pixel_size=region.pixel_size) \n",
        "            for ilon in lons for ilat in lats]\n",
        "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')\n",
        "        \n",
        "    return \n",
        "            \n",
        "def merge_tifs(files:list, fname:str, delete=False):\n",
        "    data, tfm = merge([open_tif(str(f)) for f in files])\n",
        "    data = data.squeeze()\n",
        "    fname = Path(files[0]).parent/fname\n",
        "    profile = open_tif(str(files[0])).profile\n",
        "    with rasterio.Env():\n",
        "        height, width = data.shape\n",
        "        profile.update(width=width, height=height, transform=tfm, compress='lzw')\n",
        "        with rasterio.open(str(fname), 'w', **profile) as dst:\n",
        "            dst.write(data, 1)\n",
        "    if delete:\n",
        "        for f in files: os.remove(f)\n",
        "     \n",
        "\n",
        "#export\n",
        "def filter_region(image_collection:ee.ImageCollection, region:RegionST, times:tuple, bands=None):\n",
        "    image_collection = image_collection.filterDate(times[0], times[1])\n",
        "    geometry = ee.Geometry.Rectangle(region.bbox)\n",
        "    image_collection = image_collection.filterBounds(geometry)\n",
        "    if bands is not None:\n",
        "        image_collection = image_collection.select(bands)\n",
        "    return image_collection\n",
        "\n",
        "def filter_cloudy(image_collection:ee.ImageCollection, max_cloud_fraction=0.2):\n",
        "    return image_collection.filterMetadata(\n",
        "        'CLOUDY_PIXEL_PERCENTAGE', 'not_greater_than', max_cloud_fraction)\n",
        "\n",
        "def n_least_cloudy(image_collection:ee.ImageCollection, n=5):\n",
        "    image_collection = image_collection.sort(prop='CLOUDY_PIXEL_PERCENTAGE')\n",
        "    image_collection = image_collection.toList(image_collection.size())\n",
        "    colsize = image_collection.size().getInfo()\n",
        "    if colsize < n: \n",
        "        warnings.warn(f'Total number of images in the collection {colsize} less than n={n}. Setting n={colsize}')\n",
        "        n = colsize\n",
        "    image_collection = ee.ImageCollection([ee.Image(image_collection.get(i)) for i in range(n)])\n",
        "    return image_collection\n",
        "\n",
        "def download_topography_data(R:RegionST, path_save=Path('.'), scale=None, \n",
        "                             download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    image = ee.Image('srtm90_v4')\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size)\n",
        "    if not (path_save/'srtm90_v4.elevation.tif').is_file():\n",
        "        files = []\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" +\n",
        "              f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "            url = image.getDownloadUrl(\n",
        "                {'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "            r = requests.get(url)\n",
        "            with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                f.write(r.content)\n",
        "            with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                f.extractall(str(path_save))\n",
        "                os.rename(str(path_save/'srtm90_v4.elevation.tif'),\n",
        "                          str(path_save/f'srtm90_v4.elevation_{j}.tif'))\n",
        "                files.append(str(path_save/f'srtm90_v4.elevation_{j}.tif'))\n",
        "            os.remove(str(path_save/'data.zip'))\n",
        "        merge_tifs(files, 'srtm90_v4.elevation.tif', delete=True)\n",
        "\n",
        "def download_data(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" +\n",
        "                       f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                for i in range(1, len(products)):\n",
        "                    imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                            zip_error = True\n",
        "                            for i in range(10): # Try 10 times\n",
        "                                if zip_error:\n",
        "                                    try:\n",
        "                                        url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "                                        r = requests.get(url)\n",
        "                                        with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                                            f.write(r.content)\n",
        "                                        with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                                            files = f.namelist()\n",
        "                                            f.extractall(str(path_save))\n",
        "                                        os.remove(str(path_save/'data.zip'))\n",
        "                                        zip_error = False\n",
        "                                    except:\n",
        "                                        zip_error = True\n",
        "                                        os.remove(str(path_save/'data.zip'))\n",
        "                                        time.sleep(10)\n",
        "                            if zip_error: raise Exception(f'Failed to process {url}')\n",
        "                            for f in files:\n",
        "                                f = path_save/f\n",
        "                                os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "            \n",
        "def download_data_ts(R:RegionST, products, bands, path_save, scale=None, \n",
        "                     download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    for j, R in loop:\n",
        "        region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "        # Merge products to single image collection\n",
        "        imCol = ee.ImageCollection(products[0])\n",
        "        for i in range(1, len(products)):\n",
        "            imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
        "        imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "        imCol = ee.ImageCollection(imCol)\n",
        "        colList = imCol.toList(imCol.size())\n",
        "\n",
        "        # Download each image\n",
        "        for i in range(colList.size().getInfo()):\n",
        "            image = ee.Image(colList.get(i))\n",
        "            zip_error = True\n",
        "            for i in range(10): # Try 10 times\n",
        "                if zip_error:\n",
        "                    try:\n",
        "                        url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})\n",
        "                        r = requests.get(url)\n",
        "                        with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                            f.write(r.content)\n",
        "                        with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                            files = f.namelist()\n",
        "                            f.extractall(str(path_save))\n",
        "                        os.remove(str(path_save/'data.zip'))\n",
        "                        zip_error = False\n",
        "                    except:\n",
        "                        zip_error = True\n",
        "                        os.remove(str(path_save/'data.zip'))\n",
        "                        time.sleep(10)\n",
        "            if zip_error: raise Exception(f'Failed to process {url}')\n",
        "            for f in files:\n",
        "                f = path_save/f\n",
        "                os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "                \n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "cgDoL5qty-Mg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S2**"
      ],
      "metadata": {
        "id": "qaYr__CZrM7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "    \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "# // Applies scaling factors.\n",
        "def applyScaleFactors_COPERNICUS(image):\n",
        "  opticalBands = image.divide(10000);\n",
        "  return image.addBands(srcImg = opticalBands, overwrite = True)\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_COPERNICUS(R:RegionST, products, bands, path_save, scale=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol).map(applyScaleFactors_COPERNICUS)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_COPERNICUS(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                imCol = ee.ImageCollection(imCol).map(applyScaleFactors_COPERNICUS)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "uz_pX_6Dw0Ji"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-rR0f-GmELJ"
      },
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"COPERNICUS/S2\"]  # Product id in google earth engine\n",
        "bands       = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-04-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_COPERNICUS(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts_COPERNICUS(R, products, bands, path_save, show_progress=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBTGABv5sm0d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from banet.data import open_tif\n",
        "\n",
        "brightness = 3\n",
        "im = np.concatenate([open_tif(f'data/download.{b}.tif').read() for b in bands])\n",
        "im = im.transpose(1,2,0).astype(np.float32)/10000\n",
        "plt.imshow(brightness*im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**GEDI**"
      ],
      "metadata": {
        "id": "Namgq3XVrIgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**gedi code**"
      ],
      "metadata": {
        "id": "FFi8USYVvogC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "def download_data_GEDI(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "JgcwIDqSqzqG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "qDZFInvyrBva"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"LARSE/GEDI/GEDI02_A_002_MONTHLY\"]  # Product id in google earth engine\n",
        "bands       = ['rh98'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-04-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_GEDI(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts_GEDI(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "KOre-TKTrGsW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EO1/HYPERION**"
      ],
      "metadata": {
        "id": "EP8-jdvMdPk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"EO1/HYPERION\"]  # Product id in google earth engine\n",
        "bands       = ['B104', 'B140', 'B221'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2015-03-01', \n",
        "             time_end     = '2016-07-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "-dgTB5aNdVMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR**"
      ],
      "metadata": {
        "id": "5uoTb-cE4WY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "xSM_t-7M1aOR"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_ALOS(R:RegionST, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_ALOS(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "Yr22qb0kBJPl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR\"]  # Product id in google earth engine\n",
        "bands       = ['HH', 'HV'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-02-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_ALOS(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "# download_data_ts_ALOS(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "3-giNuSezZxi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LANDSAT/LC08/C02/T2_L2**"
      ],
      "metadata": {
        "id": "DsgIikSUjWGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "    \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "# // Applies scaling factors.\n",
        "def applyScaleFactors_LANDSAT(image):\n",
        "  opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n",
        "  thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n",
        "  return image.addBands(srcImg = opticalBands, overwrite = True)\\\n",
        "              .addBands(srcImg = thermalBands, overwrite = True)\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_LANDSAT(R:RegionST, products, bands, path_save, scale=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol).map(applyScaleFactors_LANDSAT)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_LANDSAT(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "zPbn_-Fgjxin"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "# products    = [\"LANDSAT/LC09/C02/T2_L2\"]  # Product id in google earth engine\n",
        "products    = [\"LANDSAT/LC08/C02/T2_L2\"]  # Product id in google earth engine\n",
        "bands       = ['SR_B1', 'SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7','ST_B10'] # Red, Green, Blue\n",
        "\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-04-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "# download_data_LANDSAT(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "download_data_ts_LANDSAT(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "AfcCwQBujYE8"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S5P/OFFL/L3_CLOUD**"
      ],
      "metadata": {
        "id": "ks5FFYsgcK-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code**"
      ],
      "metadata": {
        "id": "5KdDjQLdcxQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "    \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        # output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_L3_CLOUD(R:RegionST, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      \n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          print(\"collecting 🔻:\",j)\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "          output.clear()\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_L3_CLOUD(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          print(\"collecting 🔻:\",j)\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "                          output.clear()\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "Kb2D2Yvvc0-q"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "BWl8x-Ynk7VU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "# products    = [\"LANDSAT/LC09/C02/T2_L2\"]  # Product id in google earth engine\n",
        "products    = [\"COPERNICUS/S5P/OFFL/L3_CLOUD\"]  # Product id in google earth engine\n",
        "bands       = ['cloud_top_height', 'cloud_base_height'] # Red, Green, Blue\n",
        "\n",
        "\n",
        "###Dataset Availability    2018-07-04T11:34:21Z–2023-01-25T09:44:34\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 1113.2, \n",
        "             time_start   = '2021-05-10', \n",
        "             time_end     = '2021-06-20')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "# download_data_L3_CLOUD(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "download_data_ts_L3_CLOUD(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J1BplFXZcMuo",
        "outputId": "24a64190-4367-4a2e-985f-67dd5b09ce4e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [1/1 06:16&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading 🔻: 0 / 579\n",
            " None\n",
            "downloading 🔻: 1 / 579\n",
            " None\n",
            "downloading 🔻: 2 / 579\n",
            " None\n",
            "downloading 🔻: 3 / 579\n",
            " None\n",
            "downloading 🔻: 4 / 579\n",
            " None\n",
            "downloading 🔻: 5 / 579\n",
            " None\n",
            "downloading 🔻: 6 / 579\n",
            " None\n",
            "downloading 🔻: 7 / 579\n",
            " None\n",
            "downloading 🔻: 8 / 579\n",
            " None\n",
            "downloading 🔻: 9 / 579\n",
            " None\n",
            "downloading 🔻: 10 / 579\n",
            " None\n",
            "downloading 🔻: 11 / 579\n",
            " None\n",
            "downloading 🔻: 12 / 579\n",
            " None\n",
            "downloading 🔻: 13 / 579\n",
            " None\n",
            "downloading 🔻: 14 / 579\n",
            " None\n",
            "downloading 🔻: 15 / 579\n",
            " None\n",
            "downloading 🔻: 16 / 579\n",
            " None\n",
            "downloading 🔻: 17 / 579\n",
            " None\n",
            "downloading 🔻: 18 / 579\n",
            " None\n",
            "downloading 🔻: 19 / 579\n",
            " None\n",
            "downloading 🔻: 20 / 579\n",
            " None\n",
            "downloading 🔻: 21 / 579\n",
            " None\n",
            "downloading 🔻: 22 / 579\n",
            " None\n",
            "downloading 🔻: 23 / 579\n",
            " None\n",
            "downloading 🔻: 24 / 579\n",
            " None\n",
            "downloading 🔻: 25 / 579\n",
            " None\n",
            "downloading 🔻: 26 / 579\n",
            " None\n",
            "downloading 🔻: 27 / 579\n",
            " None\n",
            "downloading 🔻: 28 / 579\n",
            " None\n",
            "downloading 🔻: 29 / 579\n",
            " None\n",
            "downloading 🔻: 30 / 579\n",
            " None\n",
            "downloading 🔻: 31 / 579\n",
            " None\n",
            "downloading 🔻: 32 / 579\n",
            " None\n",
            "downloading 🔻: 33 / 579\n",
            " None\n",
            "downloading 🔻: 34 / 579\n",
            " None\n",
            "downloading 🔻: 35 / 579\n",
            " None\n",
            "downloading 🔻: 36 / 579\n",
            " None\n",
            "downloading 🔻: 37 / 579\n",
            " None\n",
            "downloading 🔻: 38 / 579\n",
            " None\n",
            "downloading 🔻: 39 / 579\n",
            " None\n",
            "downloading 🔻: 40 / 579\n",
            " None\n",
            "downloading 🔻: 41 / 579\n",
            " None\n",
            "downloading 🔻: 42 / 579\n",
            " None\n",
            "downloading 🔻: 43 / 579\n",
            " None\n",
            "downloading 🔻: 44 / 579\n",
            " None\n",
            "downloading 🔻: 45 / 579\n",
            " None\n",
            "downloading 🔻: 46 / 579\n",
            " None\n",
            "downloading 🔻: 47 / 579\n",
            " None\n",
            "downloading 🔻: 48 / 579\n",
            " None\n",
            "downloading 🔻: 49 / 579\n",
            " None\n",
            "downloading 🔻: 50 / 579\n",
            " None\n",
            "downloading 🔻: 51 / 579\n",
            " None\n",
            "downloading 🔻: 52 / 579\n",
            " None\n",
            "downloading 🔻: 53 / 579\n",
            " None\n",
            "downloading 🔻: 54 / 579\n",
            " None\n",
            "downloading 🔻: 55 / 579\n",
            " None\n",
            "downloading 🔻: 56 / 579\n",
            " None\n",
            "downloading 🔻: 57 / 579\n",
            " None\n",
            "downloading 🔻: 58 / 579\n",
            " None\n",
            "downloading 🔻: 59 / 579\n",
            " None\n",
            "downloading 🔻: 60 / 579\n",
            " None\n",
            "downloading 🔻: 61 / 579\n",
            " None\n",
            "downloading 🔻: 62 / 579\n",
            " None\n",
            "downloading 🔻: 63 / 579\n",
            " None\n",
            "downloading 🔻: 64 / 579\n",
            " None\n",
            "downloading 🔻: 65 / 579\n",
            " None\n",
            "downloading 🔻: 66 / 579\n",
            " None\n",
            "downloading 🔻: 67 / 579\n",
            " None\n",
            "downloading 🔻: 68 / 579\n",
            " None\n",
            "downloading 🔻: 69 / 579\n",
            " None\n",
            "downloading 🔻: 70 / 579\n",
            " None\n",
            "downloading 🔻: 71 / 579\n",
            " None\n",
            "downloading 🔻: 72 / 579\n",
            " None\n",
            "downloading 🔻: 73 / 579\n",
            " None\n",
            "downloading 🔻: 74 / 579\n",
            " None\n",
            "downloading 🔻: 75 / 579\n",
            " None\n",
            "downloading 🔻: 76 / 579\n",
            " None\n",
            "downloading 🔻: 77 / 579\n",
            " None\n",
            "downloading 🔻: 78 / 579\n",
            " None\n",
            "downloading 🔻: 79 / 579\n",
            " None\n",
            "downloading 🔻: 80 / 579\n",
            " None\n",
            "downloading 🔻: 81 / 579\n",
            " None\n",
            "downloading 🔻: 82 / 579\n",
            " None\n",
            "downloading 🔻: 83 / 579\n",
            " None\n",
            "downloading 🔻: 84 / 579\n",
            " None\n",
            "downloading 🔻: 85 / 579\n",
            " None\n",
            "downloading 🔻: 86 / 579\n",
            " None\n",
            "downloading 🔻: 87 / 579\n",
            " None\n",
            "downloading 🔻: 88 / 579\n",
            " None\n",
            "downloading 🔻: 89 / 579\n",
            " None\n",
            "downloading 🔻: 90 / 579\n",
            " None\n",
            "downloading 🔻: 91 / 579\n",
            " None\n",
            "downloading 🔻: 92 / 579\n",
            " None\n",
            "downloading 🔻: 93 / 579\n",
            " None\n",
            "downloading 🔻: 94 / 579\n",
            " None\n",
            "downloading 🔻: 95 / 579\n",
            " None\n",
            "downloading 🔻: 96 / 579\n",
            " None\n",
            "downloading 🔻: 97 / 579\n",
            " None\n",
            "downloading 🔻: 98 / 579\n",
            " None\n",
            "downloading 🔻: 99 / 579\n",
            " None\n",
            "downloading 🔻: 100 / 579\n",
            " None\n",
            "downloading 🔻: 101 / 579\n",
            " None\n",
            "downloading 🔻: 102 / 579\n",
            " None\n",
            "downloading 🔻: 103 / 579\n",
            " None\n",
            "downloading 🔻: 104 / 579\n",
            " None\n",
            "downloading 🔻: 105 / 579\n",
            " None\n",
            "downloading 🔻: 106 / 579\n",
            " None\n",
            "downloading 🔻: 107 / 579\n",
            " None\n",
            "downloading 🔻: 108 / 579\n",
            " None\n",
            "downloading 🔻: 109 / 579\n",
            " None\n",
            "downloading 🔻: 110 / 579\n",
            " None\n",
            "downloading 🔻: 111 / 579\n",
            " None\n",
            "downloading 🔻: 112 / 579\n",
            " None\n",
            "downloading 🔻: 113 / 579\n",
            " None\n",
            "downloading 🔻: 114 / 579\n",
            " None\n",
            "downloading 🔻: 115 / 579\n",
            " None\n",
            "downloading 🔻: 116 / 579\n",
            " None\n",
            "downloading 🔻: 117 / 579\n",
            " None\n",
            "downloading 🔻: 118 / 579\n",
            " None\n",
            "downloading 🔻: 119 / 579\n",
            " None\n",
            "downloading 🔻: 120 / 579\n",
            " None\n",
            "downloading 🔻: 121 / 579\n",
            " None\n",
            "downloading 🔻: 122 / 579\n",
            " None\n",
            "downloading 🔻: 123 / 579\n",
            " None\n",
            "downloading 🔻: 124 / 579\n",
            " None\n",
            "downloading 🔻: 125 / 579\n",
            " None\n",
            "downloading 🔻: 126 / 579\n",
            " None\n",
            "downloading 🔻: 127 / 579\n",
            " None\n",
            "downloading 🔻: 128 / 579\n",
            " None\n",
            "downloading 🔻: 129 / 579\n",
            " None\n",
            "downloading 🔻: 130 / 579\n",
            " None\n",
            "downloading 🔻: 131 / 579\n",
            " None\n",
            "downloading 🔻: 132 / 579\n",
            " None\n",
            "downloading 🔻: 133 / 579\n",
            " None\n",
            "downloading 🔻: 134 / 579\n",
            " None\n",
            "downloading 🔻: 135 / 579\n",
            " None\n",
            "downloading 🔻: 136 / 579\n",
            " None\n",
            "downloading 🔻: 137 / 579\n",
            " None\n",
            "downloading 🔻: 138 / 579\n",
            " None\n",
            "downloading 🔻: 139 / 579\n",
            " None\n",
            "downloading 🔻: 140 / 579\n",
            " None\n",
            "downloading 🔻: 141 / 579\n",
            " None\n",
            "downloading 🔻: 142 / 579\n",
            " None\n",
            "downloading 🔻: 143 / 579\n",
            " None\n",
            "downloading 🔻: 144 / 579\n",
            " None\n",
            "downloading 🔻: 145 / 579\n",
            " None\n",
            "downloading 🔻: 146 / 579\n",
            " None\n",
            "downloading 🔻: 147 / 579\n",
            " None\n",
            "downloading 🔻: 148 / 579\n",
            " None\n",
            "downloading 🔻: 149 / 579\n",
            " None\n",
            "downloading 🔻: 150 / 579\n",
            " None\n",
            "downloading 🔻: 151 / 579\n",
            " None\n",
            "downloading 🔻: 152 / 579\n",
            " None\n",
            "downloading 🔻: 153 / 579\n",
            " None\n",
            "downloading 🔻: 154 / 579\n",
            " None\n",
            "downloading 🔻: 155 / 579\n",
            " None\n",
            "downloading 🔻: 156 / 579\n",
            " None\n",
            "downloading 🔻: 157 / 579\n",
            " None\n",
            "downloading 🔻: 158 / 579\n",
            " None\n",
            "downloading 🔻: 159 / 579\n",
            " None\n",
            "downloading 🔻: 160 / 579\n",
            " None\n",
            "downloading 🔻: 161 / 579\n",
            " None\n",
            "downloading 🔻: 162 / 579\n",
            " None\n",
            "downloading 🔻: 163 / 579\n",
            " None\n",
            "downloading 🔻: 164 / 579\n",
            " None\n",
            "downloading 🔻: 165 / 579\n",
            " None\n",
            "downloading 🔻: 166 / 579\n",
            " None\n",
            "downloading 🔻: 167 / 579\n",
            " None\n",
            "downloading 🔻: 168 / 579\n",
            " None\n",
            "downloading 🔻: 169 / 579\n",
            " None\n",
            "downloading 🔻: 170 / 579\n",
            " None\n",
            "downloading 🔻: 171 / 579\n",
            " None\n",
            "downloading 🔻: 172 / 579\n",
            " None\n",
            "downloading 🔻: 173 / 579\n",
            " None\n",
            "downloading 🔻: 174 / 579\n",
            " None\n",
            "downloading 🔻: 175 / 579\n",
            " None\n",
            "downloading 🔻: 176 / 579\n",
            " None\n",
            "downloading 🔻: 177 / 579\n",
            " None\n",
            "downloading 🔻: 178 / 579\n",
            " None\n",
            "downloading 🔻: 179 / 579\n",
            " None\n",
            "downloading 🔻: 180 / 579\n",
            " None\n",
            "downloading 🔻: 181 / 579\n",
            " None\n",
            "downloading 🔻: 182 / 579\n",
            " None\n",
            "downloading 🔻: 183 / 579\n",
            " None\n",
            "downloading 🔻: 184 / 579\n",
            " None\n",
            "downloading 🔻: 185 / 579\n",
            " None\n",
            "downloading 🔻: 186 / 579\n",
            " None\n",
            "downloading 🔻: 187 / 579\n",
            " None\n",
            "downloading 🔻: 188 / 579\n",
            " None\n",
            "downloading 🔻: 189 / 579\n",
            " None\n",
            "downloading 🔻: 190 / 579\n",
            " None\n",
            "downloading 🔻: 191 / 579\n",
            " None\n",
            "downloading 🔻: 192 / 579\n",
            " None\n",
            "downloading 🔻: 193 / 579\n",
            " None\n",
            "downloading 🔻: 194 / 579\n",
            " None\n",
            "downloading 🔻: 195 / 579\n",
            " None\n",
            "downloading 🔻: 196 / 579\n",
            " None\n",
            "downloading 🔻: 197 / 579\n",
            " None\n",
            "downloading 🔻: 198 / 579\n",
            " None\n",
            "downloading 🔻: 199 / 579\n",
            " None\n",
            "downloading 🔻: 200 / 579\n",
            " None\n",
            "downloading 🔻: 201 / 579\n",
            " None\n",
            "downloading 🔻: 202 / 579\n",
            " None\n",
            "downloading 🔻: 203 / 579\n",
            " None\n",
            "downloading 🔻: 204 / 579\n",
            " None\n",
            "downloading 🔻: 205 / 579\n",
            " None\n",
            "downloading 🔻: 206 / 579\n",
            " None\n",
            "downloading 🔻: 207 / 579\n",
            " None\n",
            "downloading 🔻: 208 / 579\n",
            " None\n",
            "downloading 🔻: 209 / 579\n",
            " None\n",
            "downloading 🔻: 210 / 579\n",
            " None\n",
            "downloading 🔻: 211 / 579\n",
            " None\n",
            "downloading 🔻: 212 / 579\n",
            " None\n",
            "downloading 🔻: 213 / 579\n",
            " None\n",
            "downloading 🔻: 214 / 579\n",
            " None\n",
            "downloading 🔻: 215 / 579\n",
            " None\n",
            "downloading 🔻: 216 / 579\n",
            " None\n",
            "downloading 🔻: 217 / 579\n",
            " None\n",
            "downloading 🔻: 218 / 579\n",
            " None\n",
            "downloading 🔻: 219 / 579\n",
            " None\n",
            "downloading 🔻: 220 / 579\n",
            " None\n",
            "downloading 🔻: 221 / 579\n",
            " None\n",
            "downloading 🔻: 222 / 579\n",
            " None\n",
            "downloading 🔻: 223 / 579\n",
            " None\n",
            "downloading 🔻: 224 / 579\n",
            " None\n",
            "downloading 🔻: 225 / 579\n",
            " None\n",
            "downloading 🔻: 226 / 579\n",
            " None\n",
            "downloading 🔻: 227 / 579\n",
            " None\n",
            "downloading 🔻: 228 / 579\n",
            " None\n",
            "downloading 🔻: 229 / 579\n",
            " None\n",
            "downloading 🔻: 230 / 579\n",
            " None\n",
            "downloading 🔻: 231 / 579\n",
            " None\n",
            "downloading 🔻: 232 / 579\n",
            " None\n",
            "downloading 🔻: 233 / 579\n",
            " None\n",
            "downloading 🔻: 234 / 579\n",
            " None\n",
            "downloading 🔻: 235 / 579\n",
            " None\n",
            "downloading 🔻: 236 / 579\n",
            " None\n",
            "downloading 🔻: 237 / 579\n",
            " None\n",
            "downloading 🔻: 238 / 579\n",
            " None\n",
            "downloading 🔻: 239 / 579\n",
            " None\n",
            "downloading 🔻: 240 / 579\n",
            " None\n",
            "downloading 🔻: 241 / 579\n",
            " None\n",
            "downloading 🔻: 242 / 579\n",
            " None\n",
            "downloading 🔻: 243 / 579\n",
            " None\n",
            "downloading 🔻: 244 / 579\n",
            " None\n",
            "downloading 🔻: 245 / 579\n",
            " None\n",
            "downloading 🔻: 246 / 579\n",
            " None\n",
            "downloading 🔻: 247 / 579\n",
            " None\n",
            "downloading 🔻: 248 / 579\n",
            " None\n",
            "downloading 🔻: 249 / 579\n",
            " None\n",
            "downloading 🔻: 250 / 579\n",
            " None\n",
            "downloading 🔻: 251 / 579\n",
            " None\n",
            "downloading 🔻: 252 / 579\n",
            " None\n",
            "downloading 🔻: 253 / 579\n",
            " None\n",
            "downloading 🔻: 254 / 579\n",
            " None\n",
            "downloading 🔻: 255 / 579\n",
            " None\n",
            "downloading 🔻: 256 / 579\n",
            " None\n",
            "downloading 🔻: 257 / 579\n",
            " None\n",
            "downloading 🔻: 258 / 579\n",
            " None\n",
            "downloading 🔻: 259 / 579\n",
            " None\n",
            "downloading 🔻: 260 / 579\n",
            " None\n",
            "downloading 🔻: 261 / 579\n",
            " None\n",
            "downloading 🔻: 262 / 579\n",
            " None\n",
            "downloading 🔻: 263 / 579\n",
            " None\n",
            "downloading 🔻: 264 / 579\n",
            " None\n",
            "downloading 🔻: 265 / 579\n",
            " None\n",
            "downloading 🔻: 266 / 579\n",
            " None\n",
            "downloading 🔻: 267 / 579\n",
            " None\n",
            "downloading 🔻: 268 / 579\n",
            " None\n",
            "downloading 🔻: 269 / 579\n",
            " None\n",
            "downloading 🔻: 270 / 579\n",
            " None\n",
            "downloading 🔻: 271 / 579\n",
            " None\n",
            "downloading 🔻: 272 / 579\n",
            " None\n",
            "downloading 🔻: 273 / 579\n",
            " None\n",
            "downloading 🔻: 274 / 579\n",
            " None\n",
            "downloading 🔻: 275 / 579\n",
            " None\n",
            "downloading 🔻: 276 / 579\n",
            " None\n",
            "downloading 🔻: 277 / 579\n",
            " None\n",
            "downloading 🔻: 278 / 579\n",
            " None\n",
            "downloading 🔻: 279 / 579\n",
            " None\n",
            "downloading 🔻: 280 / 579\n",
            " None\n",
            "downloading 🔻: 281 / 579\n",
            " None\n",
            "downloading 🔻: 282 / 579\n",
            " None\n",
            "downloading 🔻: 283 / 579\n",
            " None\n",
            "downloading 🔻: 284 / 579\n",
            " None\n",
            "downloading 🔻: 285 / 579\n",
            " None\n",
            "downloading 🔻: 286 / 579\n",
            " None\n",
            "downloading 🔻: 287 / 579\n",
            " None\n",
            "downloading 🔻: 288 / 579\n",
            " None\n",
            "downloading 🔻: 289 / 579\n",
            " None\n",
            "downloading 🔻: 290 / 579\n",
            " None\n",
            "downloading 🔻: 291 / 579\n",
            " None\n",
            "downloading 🔻: 292 / 579\n",
            " None\n",
            "downloading 🔻: 293 / 579\n",
            " None\n",
            "downloading 🔻: 294 / 579\n",
            " None\n",
            "downloading 🔻: 295 / 579\n",
            " None\n",
            "downloading 🔻: 296 / 579\n",
            " None\n",
            "downloading 🔻: 297 / 579\n",
            " None\n",
            "downloading 🔻: 298 / 579\n",
            " None\n",
            "downloading 🔻: 299 / 579\n",
            " None\n",
            "downloading 🔻: 300 / 579\n",
            " None\n",
            "downloading 🔻: 301 / 579\n",
            " None\n",
            "downloading 🔻: 302 / 579\n",
            " None\n",
            "downloading 🔻: 303 / 579\n",
            " None\n",
            "downloading 🔻: 304 / 579\n",
            " None\n",
            "downloading 🔻: 305 / 579\n",
            " None\n",
            "downloading 🔻: 306 / 579\n",
            " None\n",
            "downloading 🔻: 307 / 579\n",
            " None\n",
            "downloading 🔻: 308 / 579\n",
            " None\n",
            "downloading 🔻: 309 / 579\n",
            " None\n",
            "downloading 🔻: 310 / 579\n",
            " None\n",
            "downloading 🔻: 311 / 579\n",
            " None\n",
            "downloading 🔻: 312 / 579\n",
            " None\n",
            "downloading 🔻: 313 / 579\n",
            " None\n",
            "downloading 🔻: 314 / 579\n",
            " None\n",
            "downloading 🔻: 315 / 579\n",
            " None\n",
            "downloading 🔻: 316 / 579\n",
            " None\n",
            "downloading 🔻: 317 / 579\n",
            " None\n",
            "downloading 🔻: 318 / 579\n",
            " None\n",
            "downloading 🔻: 319 / 579\n",
            " None\n",
            "downloading 🔻: 320 / 579\n",
            " None\n",
            "downloading 🔻: 321 / 579\n",
            " None\n",
            "downloading 🔻: 322 / 579\n",
            " None\n",
            "downloading 🔻: 323 / 579\n",
            " None\n",
            "downloading 🔻: 324 / 579\n",
            " None\n",
            "downloading 🔻: 325 / 579\n",
            " None\n",
            "downloading 🔻: 326 / 579\n",
            " None\n",
            "downloading 🔻: 327 / 579\n",
            " None\n",
            "downloading 🔻: 328 / 579\n",
            " None\n",
            "downloading 🔻: 329 / 579\n",
            " None\n",
            "downloading 🔻: 330 / 579\n",
            " None\n",
            "downloading 🔻: 331 / 579\n",
            " None\n",
            "downloading 🔻: 332 / 579\n",
            " None\n",
            "downloading 🔻: 333 / 579\n",
            " None\n",
            "downloading 🔻: 334 / 579\n",
            " None\n",
            "downloading 🔻: 335 / 579\n",
            " None\n",
            "downloading 🔻: 336 / 579\n",
            " None\n",
            "downloading 🔻: 337 / 579\n",
            " None\n",
            "downloading 🔻: 338 / 579\n",
            " None\n",
            "downloading 🔻: 339 / 579\n",
            " None\n",
            "downloading 🔻: 340 / 579\n",
            " None\n",
            "downloading 🔻: 341 / 579\n",
            " None\n",
            "downloading 🔻: 342 / 579\n",
            " None\n",
            "downloading 🔻: 343 / 579\n",
            " None\n",
            "downloading 🔻: 344 / 579\n",
            " None\n",
            "downloading 🔻: 345 / 579\n",
            " None\n",
            "downloading 🔻: 346 / 579\n",
            " None\n",
            "downloading 🔻: 347 / 579\n",
            " None\n",
            "downloading 🔻: 348 / 579\n",
            " None\n",
            "downloading 🔻: 349 / 579\n",
            " None\n",
            "downloading 🔻: 350 / 579\n",
            " None\n",
            "downloading 🔻: 351 / 579\n",
            " None\n",
            "downloading 🔻: 352 / 579\n",
            " None\n",
            "downloading 🔻: 353 / 579\n",
            " None\n",
            "downloading 🔻: 354 / 579\n",
            " None\n",
            "downloading 🔻: 355 / 579\n",
            " None\n",
            "downloading 🔻: 356 / 579\n",
            " None\n",
            "downloading 🔻: 357 / 579\n",
            " None\n",
            "downloading 🔻: 358 / 579\n",
            " None\n",
            "downloading 🔻: 359 / 579\n",
            " None\n",
            "downloading 🔻: 360 / 579\n",
            " None\n",
            "downloading 🔻: 361 / 579\n",
            " None\n",
            "downloading 🔻: 362 / 579\n",
            " None\n",
            "downloading 🔻: 363 / 579\n",
            " None\n",
            "downloading 🔻: 364 / 579\n",
            " None\n",
            "downloading 🔻: 365 / 579\n",
            " None\n",
            "downloading 🔻: 366 / 579\n",
            " None\n",
            "downloading 🔻: 367 / 579\n",
            " None\n",
            "downloading 🔻: 368 / 579\n",
            " None\n",
            "downloading 🔻: 369 / 579\n",
            " None\n",
            "downloading 🔻: 370 / 579\n",
            " None\n",
            "downloading 🔻: 371 / 579\n",
            " None\n",
            "downloading 🔻: 372 / 579\n",
            " None\n",
            "downloading 🔻: 373 / 579\n",
            " None\n",
            "downloading 🔻: 374 / 579\n",
            " None\n",
            "downloading 🔻: 375 / 579\n",
            " None\n",
            "downloading 🔻: 376 / 579\n",
            " None\n",
            "downloading 🔻: 377 / 579\n",
            " None\n",
            "downloading 🔻: 378 / 579\n",
            " None\n",
            "downloading 🔻: 379 / 579\n",
            " None\n",
            "downloading 🔻: 380 / 579\n",
            " None\n",
            "downloading 🔻: 381 / 579\n",
            " None\n",
            "downloading 🔻: 382 / 579\n",
            " None\n",
            "downloading 🔻: 383 / 579\n",
            " None\n",
            "downloading 🔻: 384 / 579\n",
            " None\n",
            "downloading 🔻: 385 / 579\n",
            " None\n",
            "downloading 🔻: 386 / 579\n",
            " None\n",
            "downloading 🔻: 387 / 579\n",
            " None\n",
            "downloading 🔻: 388 / 579\n",
            " None\n",
            "downloading 🔻: 389 / 579\n",
            " None\n",
            "downloading 🔻: 390 / 579\n",
            " None\n",
            "downloading 🔻: 391 / 579\n",
            " None\n",
            "downloading 🔻: 392 / 579\n",
            " None\n",
            "downloading 🔻: 393 / 579\n",
            " None\n",
            "downloading 🔻: 394 / 579\n",
            " None\n",
            "downloading 🔻: 395 / 579\n",
            " None\n",
            "downloading 🔻: 396 / 579\n",
            " None\n",
            "downloading 🔻: 397 / 579\n",
            " None\n",
            "downloading 🔻: 398 / 579\n",
            " None\n",
            "downloading 🔻: 399 / 579\n",
            " None\n",
            "downloading 🔻: 400 / 579\n",
            " None\n",
            "downloading 🔻: 401 / 579\n",
            " None\n",
            "downloading 🔻: 402 / 579\n",
            " None\n",
            "downloading 🔻: 403 / 579\n",
            " None\n",
            "downloading 🔻: 404 / 579\n",
            " None\n",
            "downloading 🔻: 405 / 579\n",
            " None\n",
            "downloading 🔻: 406 / 579\n",
            " None\n",
            "downloading 🔻: 407 / 579\n",
            " None\n",
            "downloading 🔻: 408 / 579\n",
            " None\n",
            "downloading 🔻: 409 / 579\n",
            " None\n",
            "downloading 🔻: 410 / 579\n",
            " None\n",
            "downloading 🔻: 411 / 579\n",
            " None\n",
            "downloading 🔻: 412 / 579\n",
            " None\n",
            "downloading 🔻: 413 / 579\n",
            " None\n",
            "downloading 🔻: 414 / 579\n",
            " None\n",
            "downloading 🔻: 415 / 579\n",
            " None\n",
            "downloading 🔻: 416 / 579\n",
            " None\n",
            "downloading 🔻: 417 / 579\n",
            " None\n",
            "downloading 🔻: 418 / 579\n",
            " None\n",
            "downloading 🔻: 419 / 579\n",
            " None\n",
            "downloading 🔻: 420 / 579\n",
            " None\n",
            "downloading 🔻: 421 / 579\n",
            " None\n",
            "downloading 🔻: 422 / 579\n",
            " None\n",
            "downloading 🔻: 423 / 579\n",
            " None\n",
            "downloading 🔻: 424 / 579\n",
            " None\n",
            "downloading 🔻: 425 / 579\n",
            " None\n",
            "downloading 🔻: 426 / 579\n",
            " None\n",
            "downloading 🔻: 427 / 579\n",
            " None\n",
            "downloading 🔻: 428 / 579\n",
            " None\n",
            "downloading 🔻: 429 / 579\n",
            " None\n",
            "downloading 🔻: 430 / 579\n",
            " None\n",
            "downloading 🔻: 431 / 579\n",
            " None\n",
            "downloading 🔻: 432 / 579\n",
            " None\n",
            "downloading 🔻: 433 / 579\n",
            " None\n",
            "downloading 🔻: 434 / 579\n",
            " None\n",
            "downloading 🔻: 435 / 579\n",
            " None\n",
            "downloading 🔻: 436 / 579\n",
            " None\n",
            "downloading 🔻: 437 / 579\n",
            " None\n",
            "downloading 🔻: 438 / 579\n",
            " None\n",
            "downloading 🔻: 439 / 579\n",
            " None\n",
            "downloading 🔻: 440 / 579\n",
            " None\n",
            "downloading 🔻: 441 / 579\n",
            " None\n",
            "downloading 🔻: 442 / 579\n",
            " None\n",
            "downloading 🔻: 443 / 579\n",
            " None\n",
            "downloading 🔻: 444 / 579\n",
            " None\n",
            "downloading 🔻: 445 / 579\n",
            " None\n",
            "downloading 🔻: 446 / 579\n",
            " None\n",
            "downloading 🔻: 447 / 579\n",
            " None\n",
            "downloading 🔻: 448 / 579\n",
            " None\n",
            "downloading 🔻: 449 / 579\n",
            " None\n",
            "downloading 🔻: 450 / 579\n",
            " None\n",
            "downloading 🔻: 451 / 579\n",
            " None\n",
            "downloading 🔻: 452 / 579\n",
            " None\n",
            "downloading 🔻: 453 / 579\n",
            " None\n",
            "downloading 🔻: 454 / 579\n",
            " None\n",
            "downloading 🔻: 455 / 579\n",
            " None\n",
            "downloading 🔻: 456 / 579\n",
            " None\n",
            "downloading 🔻: 457 / 579\n",
            " None\n",
            "downloading 🔻: 458 / 579\n",
            " None\n",
            "downloading 🔻: 459 / 579\n",
            " None\n",
            "downloading 🔻: 460 / 579\n",
            " None\n",
            "downloading 🔻: 461 / 579\n",
            " None\n",
            "downloading 🔻: 462 / 579\n",
            " None\n",
            "downloading 🔻: 463 / 579\n",
            " None\n",
            "downloading 🔻: 464 / 579\n",
            " None\n",
            "downloading 🔻: 465 / 579\n",
            " None\n",
            "downloading 🔻: 466 / 579\n",
            " None\n",
            "downloading 🔻: 467 / 579\n",
            " None\n",
            "downloading 🔻: 468 / 579\n",
            " None\n",
            "downloading 🔻: 469 / 579\n",
            " None\n",
            "downloading 🔻: 470 / 579\n",
            " None\n",
            "downloading 🔻: 471 / 579\n",
            " None\n",
            "downloading 🔻: 472 / 579\n",
            " None\n",
            "downloading 🔻: 473 / 579\n",
            " None\n",
            "downloading 🔻: 474 / 579\n",
            " None\n",
            "downloading 🔻: 475 / 579\n",
            " None\n",
            "downloading 🔻: 476 / 579\n",
            " None\n",
            "downloading 🔻: 477 / 579\n",
            " None\n",
            "downloading 🔻: 478 / 579\n",
            " None\n",
            "downloading 🔻: 479 / 579\n",
            " None\n",
            "downloading 🔻: 480 / 579\n",
            " None\n",
            "downloading 🔻: 481 / 579\n",
            " None\n",
            "downloading 🔻: 482 / 579\n",
            " None\n",
            "downloading 🔻: 483 / 579\n",
            " None\n",
            "downloading 🔻: 484 / 579\n",
            " None\n",
            "downloading 🔻: 485 / 579\n",
            " None\n",
            "downloading 🔻: 486 / 579\n",
            " None\n",
            "downloading 🔻: 487 / 579\n",
            " None\n",
            "downloading 🔻: 488 / 579\n",
            " None\n",
            "downloading 🔻: 489 / 579\n",
            " None\n",
            "downloading 🔻: 490 / 579\n",
            " None\n",
            "downloading 🔻: 491 / 579\n",
            " None\n",
            "downloading 🔻: 492 / 579\n",
            " None\n",
            "downloading 🔻: 493 / 579\n",
            " None\n",
            "downloading 🔻: 494 / 579\n",
            " None\n",
            "downloading 🔻: 495 / 579\n",
            " None\n",
            "downloading 🔻: 496 / 579\n",
            " None\n",
            "downloading 🔻: 497 / 579\n",
            " None\n",
            "downloading 🔻: 498 / 579\n",
            " None\n",
            "downloading 🔻: 499 / 579\n",
            " None\n",
            "downloading 🔻: 500 / 579\n",
            " None\n",
            "downloading 🔻: 501 / 579\n",
            " None\n",
            "downloading 🔻: 502 / 579\n",
            " None\n",
            "downloading 🔻: 503 / 579\n",
            " None\n",
            "downloading 🔻: 504 / 579\n",
            " None\n",
            "downloading 🔻: 505 / 579\n",
            " None\n",
            "downloading 🔻: 506 / 579\n",
            " None\n",
            "downloading 🔻: 507 / 579\n",
            " None\n",
            "downloading 🔻: 508 / 579\n",
            " None\n",
            "downloading 🔻: 509 / 579\n",
            " None\n",
            "downloading 🔻: 510 / 579\n",
            " None\n",
            "downloading 🔻: 511 / 579\n",
            " None\n",
            "downloading 🔻: 512 / 579\n",
            " None\n",
            "downloading 🔻: 513 / 579\n",
            " None\n",
            "downloading 🔻: 514 / 579\n",
            " None\n",
            "downloading 🔻: 515 / 579\n",
            " None\n",
            "downloading 🔻: 516 / 579\n",
            " None\n",
            "downloading 🔻: 517 / 579\n",
            " None\n",
            "downloading 🔻: 518 / 579\n",
            " None\n",
            "downloading 🔻: 519 / 579\n",
            " None\n",
            "downloading 🔻: 520 / 579\n",
            " None\n",
            "downloading 🔻: 521 / 579\n",
            " None\n",
            "downloading 🔻: 522 / 579\n",
            " None\n",
            "downloading 🔻: 523 / 579\n",
            " None\n",
            "downloading 🔻: 524 / 579\n",
            " None\n",
            "downloading 🔻: 525 / 579\n",
            " None\n",
            "downloading 🔻: 526 / 579\n",
            " None\n",
            "downloading 🔻: 527 / 579\n",
            " None\n",
            "downloading 🔻: 528 / 579\n",
            " None\n",
            "downloading 🔻: 529 / 579\n",
            " None\n",
            "downloading 🔻: 530 / 579\n",
            " None\n",
            "downloading 🔻: 531 / 579\n",
            " None\n",
            "downloading 🔻: 532 / 579\n",
            " None\n",
            "downloading 🔻: 533 / 579\n",
            " None\n",
            "downloading 🔻: 534 / 579\n",
            " None\n",
            "downloading 🔻: 535 / 579\n",
            " None\n",
            "downloading 🔻: 536 / 579\n",
            " None\n",
            "downloading 🔻: 537 / 579\n",
            " None\n",
            "downloading 🔻: 538 / 579\n",
            " None\n",
            "downloading 🔻: 539 / 579\n",
            " None\n",
            "downloading 🔻: 540 / 579\n",
            " None\n",
            "downloading 🔻: 541 / 579\n",
            " None\n",
            "downloading 🔻: 542 / 579\n",
            " None\n",
            "downloading 🔻: 543 / 579\n",
            " None\n",
            "downloading 🔻: 544 / 579\n",
            " None\n",
            "downloading 🔻: 545 / 579\n",
            " None\n",
            "downloading 🔻: 546 / 579\n",
            " None\n",
            "downloading 🔻: 547 / 579\n",
            " None\n",
            "downloading 🔻: 548 / 579\n",
            " None\n",
            "downloading 🔻: 549 / 579\n",
            " None\n",
            "downloading 🔻: 550 / 579\n",
            " None\n",
            "downloading 🔻: 551 / 579\n",
            " None\n",
            "downloading 🔻: 552 / 579\n",
            " None\n",
            "downloading 🔻: 553 / 579\n",
            " None\n",
            "downloading 🔻: 554 / 579\n",
            " None\n",
            "downloading 🔻: 555 / 579\n",
            " None\n",
            "downloading 🔻: 556 / 579\n",
            " None\n",
            "downloading 🔻: 557 / 579\n",
            " None\n",
            "downloading 🔻: 558 / 579\n",
            " None\n",
            "downloading 🔻: 559 / 579\n",
            " None\n",
            "downloading 🔻: 560 / 579\n",
            " None\n",
            "downloading 🔻: 561 / 579\n",
            " None\n",
            "downloading 🔻: 562 / 579\n",
            " None\n",
            "downloading 🔻: 563 / 579\n",
            " None\n",
            "downloading 🔻: 564 / 579\n",
            " None\n",
            "downloading 🔻: 565 / 579\n",
            " None\n",
            "downloading 🔻: 566 / 579\n",
            " None\n",
            "downloading 🔻: 567 / 579\n",
            " None\n",
            "downloading 🔻: 568 / 579\n",
            " None\n",
            "downloading 🔻: 569 / 579\n",
            " None\n",
            "downloading 🔻: 570 / 579\n",
            " None\n",
            "downloading 🔻: 571 / 579\n",
            " None\n",
            "downloading 🔻: 572 / 579\n",
            " None\n",
            "downloading 🔻: 573 / 579\n",
            " None\n",
            "downloading 🔻: 574 / 579\n",
            " None\n",
            "downloading 🔻: 575 / 579\n",
            " None\n",
            "downloading 🔻: 576 / 579\n",
            " None\n",
            "downloading 🔻: 577 / 579\n",
            " None\n",
            "downloading 🔻: 578 / 579\n",
            " None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S1_GRD**"
      ],
      "metadata": {
        "id": "P7O0vJ9p-2H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_S1_GRD(R:RegionST, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_S1_GRD(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    # print(sR)\n",
        "    fsaves = []\n",
        "    #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "        print(R)\n",
        "        region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "        # Merge products to single image collection\n",
        "        imCol = ee.ImageCollection(products[0])\n",
        "        imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "        if max_cloud_fraction is not None:\n",
        "            imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "        if use_least_cloudy is not None:\n",
        "            imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "        im = imCol.median()\n",
        "        imCol = ee.ImageCollection([im])\n",
        "        colList = imCol.toList(imCol.size())\n",
        "        # info = colList.getInfo()\n",
        "        # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "        # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "        # Download each image\n",
        "        for i in range(colList.size().getInfo()):\n",
        "            image = ee.Image(colList.get(i))\n",
        "            fname = 'download'\n",
        "            #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "            fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "            fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "            fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "            fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "            if not fnames_full:\n",
        "                fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                if not fnames_partial:\n",
        "                  url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list) \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    #files = np.unique(fsaves) \n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n"
      ],
      "metadata": {
        "id": "rafg5yLa-5FB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(xxx):\n",
        "  print(\"Downloading🔻\")\n",
        "  j = (i for i, item in enumerate(uni) if item == xxx)\n",
        "  R = xxx\n",
        "  region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "  # Merge products to single image collection\n",
        "  imCol = ee.ImageCollection(products[0])\n",
        "  imCol = filter_region(imCol, R, times=time_window, bands=bands)\n",
        "  im = imCol.median()\n",
        "  imCol = ee.ImageCollection([im])\n",
        "  colList = imCol.toList(imCol.size())\n",
        "  # info = colList.getInfo()\n",
        "  # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "  # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "  # Download each image\n",
        "  for i in range(colList.size().getInfo()):\n",
        "      image = ee.Image(colList.get(i))\n",
        "      fname = 'download'\n",
        "      #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "      fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "      fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "      fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "      fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "      if not fnames_full:\n",
        "          fsaves.append([path_save/f for f in fnames_partial0])\n",
        "          if not fnames_partial:\n",
        "              zip_error = True\n",
        "              for i in range(10): # Try 10 times\n",
        "                  if zip_error:\n",
        "                      try:\n",
        "                          url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "                          print(url)\n",
        "                          r = requests.get(url)\n",
        "                          with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                              f.write(r.content)\n",
        "                          with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                              files = f.namelist()\n",
        "                              f.extractall(str(path_save))\n",
        "                          os.remove(str(path_save/'data.zip'))\n",
        "                          zip_error = False\n",
        "                      except:\n",
        "                          zip_error = True\n",
        "                          os.remove(str(path_save/'data.zip'))\n",
        "                          time.sleep(2)\n",
        "              if zip_error: raise Exception(f'Failed to process {url}')\n",
        "              for f in files:\n",
        "                  f = path_save/f\n",
        "                  os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        # output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "uni = []\n",
        "fsaves = []\n",
        "def download_data_S1_GRD(R:RegionST, times, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    # print(sR)\n",
        "    \n",
        "    uni.clear()\n",
        "    uni.append(sR)\n",
        "    \n",
        "    download_parallel(sR) \n",
        "\n",
        "        \n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    #files = np.unique(fsaves) \n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n"
      ],
      "metadata": {
        "id": "fR_g_Ra9NkAQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb8Gdmr5_3xT",
        "outputId": "38a9366c-fd46-45af-f611-4ad8ce5e92f5"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading🔻\n",
            "Downloading🔻\n",
            "Downloading🔻\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR\"]  # Product id in google earth engine\n",
        "bands       = ['HH', 'HV'] # Red, Green, Blue\n",
        "scale       = 10\n",
        "\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = scale, \n",
        "             time_start   = '2020-02-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_S1_GRD(R, time_window, products, bands, path_save, show_progress=True)\n",
        "\n",
        "# download_data_ts_S1_GRD(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlBRU5LLABBv",
        "outputId": "3dd62e3a-db43-4201-ffe4-d86f213b5ffc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading🔻\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/7123ee0b859884a28108ef7f55a68bf3-5bc04d3e98a750a28f3abd80a3762f84:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/e701af31f61f8fa9d3705c7c3046d110-8402fcb3f2d395fc33e8fd57345a3cdc:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/15c7ff834322d466e292a99b91bb4dd4-9fdafdc56fe0fa99511371372e129e24:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/339df8e53b45821bfe8075b3e3e35ec2-56ccafd5606fd3f4ab7dbbb9a5d6469d:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/937b1fc0582938909269174e0eb2d7b6-3398a4b5769b3ce675b98be90b0326dd:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b4ea16087fe8159e58a961e424bb7be1-bac00627ec787b076a86b5fbb2b8ac43:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/8c1cf63f614baa970959e7f1e081c786-c65e2065d2b8da724d3681154712da73:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/168cfff57b3e7ff131346b729363ced7-d2a14c36e3ce32c9bd35f87b442b1779:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/96f2f5f33ece475bcf1481f3ba3db430-d50882b248148cd2e7683b86c2ee474e:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/8d55befb8dfbc1a43012f1d9123e9a5f-74c203ce4910835da97ea054c0c165cb:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/437be2d028e66c7aaa6511ff66e8682e-8a794728fa7529146d42418b7b58a6f6:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/e4d056d894c09ddf8af403aba67ff1c9-f8c63f25f0b131c01acadf386170e726:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/becd30c13f024b90c9b8c854c390dbea-624ae9b1d6861cff2e2dc610e8f2bec2:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/027fc199298b6bb8a58fa50df0aa8db6-94370aa26544fdb2676a6376a7ef11b3:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ad70a76251c34591642d168506e5c41e-33baa8c4b989483e764402ee385e0396:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/dfd732269f5b35217899370aabee09ea-ac8d4648b0220fa58ae6de5d8d06d129:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/e7844d1db05ded2884ec61034dc750fa-066fb2c9e2f63fe43ea346e607020fa6:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/5a4c78917e7f4d73b13abf8709a927d7-46392cb874c05c6eeaecb90c54c94e67:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/bbbfb2ae4955fe30d82f27f5e99b7657-7405c6113761bef034b5c6b773a3d00a:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ef0f40a2f95dd7294eb14f2d617bce40-94c0db792edb38edc5b9d5b08a2fb31f:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/59b1d36ed5be1f983f85198884831a1a-ebc9aa578daff09b9dc5a80d0babbd83:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b031101709c0f946a02f6e7351e5a264-9ba49acb6a44a481e13ebad67536bddd:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/e123380fe6f5c2978ae71cb8e2e51e2e-d0623d807a9a9833b2d923afede24d65:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/6b1f292f2400bc7162f67b035a6e95f9-368665de61567378d85913d9f5bd80c1:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/fde159ff06da6d38da2dcf4a1694564f-8bd3da60b6a477ae83972b69ca4d1b7b:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/be7a49866e133f750a97c4b469c52e55-8b2402e291ef1e77dfc3709e92d24d9d:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/251da5c1b9a104cee33cb9f8ef5d83d5-332924ffcb39515f54d7c8417a1392ab:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/437d6643589b5c7b15f2d5b67eef8ea2-782d6acadfe668d4a4ed2e1d5610f13a:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 \n",
            "None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/a56ce49d45ca04dce5b8e207d5964e3f-e5471bd29cb680c6e7da6819ab6530e5:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/8f53e527ca9eefc7bbbfcb3016271fe4-39793968e09d9c8f55c755ad7ef28c54:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/31214ea534a6f752c563283714c1af60-c1e1c5a1f130bd52ef92137186036c2e:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/c22c93b3e9766d527b60d431502f23b5-5de438612aaf9ab810927520b17fea4e:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/5fd78db3ffb2a0f46ec9fd0a435f05d9-923e75951a0e5a6a187e37ef6ae56506:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/745308a110283bcd68330f6915f1d66e-60cc784bb1f3586516f16d223107300a:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/251ae6d61be1d05d11529316bb8754bd-5ac9c6f6a89adbf8eaa7099c93154caf:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ff8df9ad059b7c5d60f6adf246dac886-221477a3e156dd6e74e0da65cd11fb32:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ee324ed86b8b734044ece9efc8bde462-458f50f7b3e6788396e124873593137b:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/328693a147b83aa6eb0b2fe44cf688e5-ba76891d2132d6ced97b321596bd2a8a:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/9ccf8190044b31152f26ac289b7821ad-c8f046c0555119393dd20773b1e47244:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 \n",
            "None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/f94f38977aea07deb94c6f6b6e622706-9e126973d528d9a4fafef60744993816:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/97377586a672af95f4b166ca41435554-e1f330ccefbf8909ac30d95abef326fb:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ad8a88237387d1ecd4264586a55d2dbc-69b15e98fbad155fee26eeb68df29b4f:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/7b1df726750cdf64c236a3990afc6a55-0db8f13c5d0c6984dc759dc2318b0b26:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b9846000593706f118e892496f105734-a5bf60bde18cebc26c1f4bce144df873:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/48af421117cf5c6fd14383bb5bdb8863-e5a708c4911a9b0705fc127fe6c13b64:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/0126ae1eb83df505412288efa2223f5c-d93cbcd6308c803b1d6423254ffc517b:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/82491bac5e2b23b1c55057e8c425e0b2-25226b8fdfa1bb993689ea132eee4df1:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/e3c298e75f26a778ceaf8784ea993f64-eb81be0ecacb95a0e1f0cd973e090199:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/d82c15851ab060166d4e3ab648e4af1a-a889cf4282812d4758b1deb6df3ab8f3:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/cbeb57a8f72089968857879e62fda36a-ef59204d274ababb745723600a819c90:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/897a2dd31668652bcf13eb1d085b2c0b-968e09938040a4430fa845322460bcee:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/c317ab9b98fa32adbbf57aafa2e01dc9-4ac15f89b16885e59e4b9ec62fd003c7:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/3416ab6af716fc4e4610fcf0b43452c5-24b001cf0da475f800fc4a4cae509a89:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/95c0e695f4d4462a9f0c3478e9ddf254-c49fbdd0c1aa87ba5de1533dbe515f50:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/1368517a48cce556e9b6da2aa831d7eb-a3f3628f1ea9a89abf4ffd81990ec9bf:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/2117e587cbbabe655cefee8c262b6d1d-25cf25ac98f9dca6bf85e75469217dc8:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/29010e36b5c00e3c096e5bc6a96efc31-45ffc1752dd24ffde74f8d870582ca50:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/bd51f4c996dc8adc6025cb523963e60a-4c58cf64064c65fbf78bd14079e6d898:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/870f4162f01bc89e0fcefcc38edffef9-0f138e0bf50235266a5c4916925df5b2:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/e7219ba79a75e7f84669bb09198cb8fd-d7a62769aae9577dc22041579ec9ea66:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/1ed26bfe3b3bdd52c06000ea2f164898-79f9fffe73370073c1217087fe1dad21:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/189a28589e52766a19f70187942cd626-82114bf806e56805d11a2e9692880445:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/1921cef716c54443daf94b82f48833ec-cd431e0b92ae475cbc27d1ffa02b7dbd:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/357fe4a2886b1a45413f0e6faddba0d4-a65ebbdc9cf199e8f142308fc6917469:getPixels\n",
            "Downloading🔻 \n",
            "None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/7eff2a4c3e2425d847283a5f394dd28b-c08353a1f62016c02f11e59827fa9490:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/d5051b1f4e7418f0a694bd3bb223c21d-ee4022ddcf36169e1b11b10b7950cb99:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/f7621c5d6bae0e2f7d83b75c16573b9a-f58ceddb7c10f94d1925d93a872be1c9:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 \n",
            "None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/0c876b11ea9bbb77951147a688562399-025cebffa8d29bc3d2a16655230d312b:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/5d0f62843ae730125d7505fc9d99d410-7bd3f3d81666c2b8a240c5c5f5f5fc71:getPixels\n",
            "Downloading🔻 \n",
            "None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/87145791105d81768e7b61562c1caf66-7b9a1a0d3472663ab34fbb0a317bdfbe:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/791e82e51a02d9d49f1c83da1846fbbd-bc5a4ca62abf38d6a8547937e911064c:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/64f9b755fb101aef51849cc0bc47deca-9082ae09794d509b4b41fd91ea46b03c:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 \n",
            "None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/bd5ff17fb3d8177366b0af87f8f828cb-cc46dc99771271f6c38479d68217a9c1:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ab398a39bcfbb44317ea6d6b4bd9e6c7-8f05f8c82f39c8dce54f82520c249471:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/c1d1cd0736b678f0b90440c945451348-6da517cf3d07632890d56271730963c4:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/14205b62aa38091a5372be8aa3670cae-37213ef3f8bea21cd443c4f1b07e7284:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/fbb5c103b68e2675c40b35a055a10f52-bc5245cc1b4ba5eba8d5058afb346346:getPixels\n",
            "Downloading🔻 \n",
            "None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b6c83008ea489f3d98511bedd7590cfb-5956fd4a6eb1cdbcf3465af3b60f8720:getPixels\n",
            "Downloading🔻 \n",
            "None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/8a97e3d8bdaa18144d369b9da34431c9-50d924b2641e6824375cbaf410b5ade7:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b0fc9564b3ca08435833cfcc1ba49e9e-caec5ce8814f59461fb6524bde75c81b:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/a75bb2c3237feae2c332c639be47c304-5d7671489b48d9b0ce262409d14c7c6c:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/a73c52b81e381844401e02ac91676607-2a5313888a7c0ae9b12326c698ec29c1:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/c89c54990cde2dd480f3e15842e04a23-2af5f76c0c918e1cc42ba4d5a50ac835:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/b5cc68f0e5863e05047482f81c5698c6-d0f4f6bd9a3c14e21e8828abb1de894c:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/33171621f9434ef7a076657673cf7124-eb5ade81cf0c5dc7f725f14bb66a4ab3:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/5bc1a3d6b8aa294c0b783171a675111b-8177d38d9668171ca12bf657e35672c9:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/7f02286ddfc3e27bf3606f1663c43fb7-7baac8d5cc633fab4169c4dbe301e6cb:getPixels\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/3ee8f7df1082bbec3df52b56a1994df3-2ccfb14778778fa411aa50c1d28b24e7:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ebd1c569c5868bfa4526a927ca1bc617-460d709025fed7246203cdcaae444031:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/62e26bf000cb17a82c6dc895c8dd4cae-dca63aa044e6b81da3a59384eb3ed3cd:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/4c329b1a2eb20a7c92ce292b45f0b7fb-6521bbe51574c26b99c8e041a5689dd2:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/0b86e9adf26560e778b0ade0fb4c51fe-c71d66e1278eb43af30a03a42b6f5438:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/ea6c143b0b473d270cd9627512845f62-47e3ce1d0a6b9343e288aab6a8a3718f:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/d4c786b59499790dad7bbb0af1387305-e08261e70fd38fc7b3771ec1587a5c97:getPixels\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻 None\n",
            "\n",
            "Downloading🔻\n",
            " None\n",
            "Downloading🔻 None\n",
            "\n",
            " None\n"
          ]
        }
      ]
    }
  ]
}