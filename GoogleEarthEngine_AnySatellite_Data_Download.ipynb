{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/test2023/blob/main/GoogleEarthEngine_AnySatellite_Data_Download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy4HceOol_h1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ae246d-1381-4f11-ba6c-70fd32387845"
      },
      "source": [
        "# Install the library\n",
        "!pip -q install FireHR==0.1.2 pyhdf==0.10.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/149.1 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m143.4/149.1 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.3/193.3 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.6/133.6 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.1/439.1 KB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.0/424.0 KB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.2/365.2 KB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jsonschema 4.3.3 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for pyhdf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cdsapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook~=5.7.16, but you have notebook 6.5.2 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=6.0.4, but you have tornado 6.2 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.0.3 which is incompatible.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.3.9 which is incompatible.\n",
            "confection 0.0.3 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKyI9oFmmvpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c04122-0b91-4411-b242-5342646287ae"
      },
      "source": [
        "# Authenticate to use Google Earth Engine API\n",
        "import ee\n",
        "ee.Authenticate()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
            "\n",
            "    https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=Q6GUNn7WbOjcBFz6K8ZuaYL6pAPV14nzriJsdwoGK90&tc=QMFsTYtFIEuJTuCRNZd0c9JakjyHO4VwsdUgim46Oqk&cc=hASbZxW8Upt_Oo01qAT4O8_YyF9LTfxLiyqL4kbAMAA\n",
            "\n",
            "The authorization workflow will generate a code, which you should paste in the box below.\n",
            "Enter verification code: 4/1AWtgzh51tPWo4CMZzmXJjU8vgISDan8FMjFXElHcyi9RinuAFTCwJSFaufg\n",
            "\n",
            "Successfully saved authorization token.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z87ijnz_4gMy",
        "outputId": "99b213ca-91b5-49e8-8da7-2576e32bf4ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/OUT/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IypPoKGP5LrP",
        "outputId": "967c0fa4-3b82-4202-e883-97c01ccbb621"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/OUT/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**main code**"
      ],
      "metadata": {
        "id": "C1hnMrqQklVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bare code**"
      ],
      "metadata": {
        "id": "_bmbtllFy7KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "     \n",
        "from FireHR.data import *\n",
        "#export\n",
        "class RegionST(Region):\n",
        "    \"Defines a region in space and time with a name, a bounding box and the pixel size.\"\n",
        "    def __init__(self, name:str, bbox:list, pixel_size:float=None, scale_meters:int=None,\n",
        "                 time_start:str=None, time_end:str=None, time_freq:str='D', time_margin:int=0,\n",
        "                 shape:tuple=None, epsg=4326):\n",
        "        if scale_meters is not None and pixel_size is not None: \n",
        "            raise Exception('Either pixel_size or scale_meters must be set to None.')\n",
        "        self.name = name\n",
        "        self.bbox = rasterio.coords.BoundingBox(*bbox) # left, bottom, right, top\n",
        "        if pixel_size is not None:\n",
        "            self.pixel_size = pixel_size\n",
        "        else:\n",
        "            self.pixel_size = scale_meters/111000\n",
        "        self.epsg         = epsg\n",
        "        self.scale_meters = scale_meters\n",
        "        self._shape       = shape\n",
        "        self.time_start   = pd.Timestamp(str(time_start))\n",
        "        self.time_end     = pd.Timestamp(str(time_end))\n",
        "        self.time_margin  = time_margin\n",
        "        self.time_freq    = time_freq\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        \"Shape of the region (height, width)\"\n",
        "        if self._shape is None:\n",
        "            return (self.height, self.width)\n",
        "        else: return self._shape\n",
        "        \n",
        "    @property\n",
        "    def times(self):\n",
        "        \"Property that computes the date_range for the region.\"\n",
        "        tstart = self.time_start - pd.Timedelta(days=self.time_margin)\n",
        "        tend = self.time_end + pd.Timedelta(days=self.time_margin)\n",
        "        return pd.date_range(tstart, tend, freq=self.time_freq)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file, time_start=None, time_end=None):\n",
        "        \"Loads region information from json file\"\n",
        "        with open(file, 'r') as f:\n",
        "            args = json.load(f)\n",
        "        if time_start is None:\n",
        "            time_start = args['time_start']\n",
        "        if time_end is None:\n",
        "            time_end = args['time_end']\n",
        "        return cls(args['name'], args['bbox'], args['pixel_size'],\n",
        "                   time_start=time_start, time_end=time_end)\n",
        "    \n",
        "def extract_region(df_row, cls=Region):\n",
        "    \"Create Region object from a row of the metadata dataframe.\"\n",
        "    if issubclass(cls, RegionST):\n",
        "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size, \n",
        "                   df_row.time_start, df_row.time_end)\n",
        "    elif issubclass(cls, Region):\n",
        "        return cls(df_row.event_id, df_row.bbox, df_row.pixel_size)\n",
        "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')\n",
        "     \n",
        "\n",
        "#export\n",
        "def coords2bbox(lon, lat, pixel_size): \n",
        "    return [lon.min(), lat.min(), lon.max()+pixel_size, lat.max()+pixel_size]\n",
        "\n",
        "def split_region(region:RegionST, size:int, cls=Region):\n",
        "    lon, lat = region.coords()\n",
        "    Nlon = (len(lon)//size)*size\n",
        "    Nlat = (len(lat)//size)*size\n",
        "    lons = [*lon[:Nlon].reshape(-1, size), lon[Nlon:][None]]\n",
        "    lats = [*lat[:Nlat].reshape(-1, size), lat[Nlat:][None]]\n",
        "    if len(lats[-1].reshape(-1)) == 0 and len(lons[-1].reshape(-1)) == 0:\n",
        "        lons = lons[:-1]\n",
        "        lats = lats[:-1]\n",
        "    #lons = lon.reshape(-1, size)\n",
        "    #lats = lat.reshape(-1, size)\n",
        "    if issubclass(cls, RegionST):\n",
        "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), \n",
        "                    pixel_size=region.pixel_size, time_start=region.time_start,\n",
        "                    time_end=region.time_end, time_freq=region.time_freq,\n",
        "                    time_margin=region.time_margin) for ilon in lons for ilat in lats]\n",
        "    elif issubclass(cls, Region):\n",
        "        return [cls('', coords2bbox(ilon, ilat, region.pixel_size), pixel_size=region.pixel_size) \n",
        "            for ilon in lons for ilat in lats]\n",
        "    else: raise NotImplemented('cls must be one of the following [Region, RegionST]')\n",
        "        \n",
        "    return \n",
        "            \n",
        "def merge_tifs(files:list, fname:str, delete=False):\n",
        "    data, tfm = merge([open_tif(str(f)) for f in files])\n",
        "    data = data.squeeze()\n",
        "    fname = Path(files[0]).parent/fname\n",
        "    profile = open_tif(str(files[0])).profile\n",
        "    with rasterio.Env():\n",
        "        height, width = data.shape\n",
        "        profile.update(width=width, height=height, transform=tfm, compress='lzw')\n",
        "        with rasterio.open(str(fname), 'w', **profile) as dst:\n",
        "            dst.write(data, 1)\n",
        "    if delete:\n",
        "        for f in files: os.remove(f)\n",
        "     \n",
        "\n",
        "#export\n",
        "def filter_region(image_collection:ee.ImageCollection, region:RegionST, times:tuple, bands=None):\n",
        "    image_collection = image_collection.filterDate(times[0], times[1])\n",
        "    geometry = ee.Geometry.Rectangle(region.bbox)\n",
        "    image_collection = image_collection.filterBounds(geometry)\n",
        "    if bands is not None:\n",
        "        image_collection = image_collection.select(bands)\n",
        "    return image_collection\n",
        "\n",
        "def filter_cloudy(image_collection:ee.ImageCollection, max_cloud_fraction=0.2):\n",
        "    return image_collection.filterMetadata(\n",
        "        'CLOUDY_PIXEL_PERCENTAGE', 'not_greater_than', max_cloud_fraction)\n",
        "\n",
        "def n_least_cloudy(image_collection:ee.ImageCollection, n=5):\n",
        "    image_collection = image_collection.sort(prop='CLOUDY_PIXEL_PERCENTAGE')\n",
        "    image_collection = image_collection.toList(image_collection.size())\n",
        "    colsize = image_collection.size().getInfo()\n",
        "    if colsize < n: \n",
        "        warnings.warn(f'Total number of images in the collection {colsize} less than n={n}. Setting n={colsize}')\n",
        "        n = colsize\n",
        "    image_collection = ee.ImageCollection([ee.Image(image_collection.get(i)) for i in range(n)])\n",
        "    return image_collection\n",
        "\n",
        "def download_topography_data(R:RegionST, path_save=Path('.'), scale=None, \n",
        "                             download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    image = ee.Image('srtm90_v4')\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size)\n",
        "    if not (path_save/'srtm90_v4.elevation.tif').is_file():\n",
        "        files = []\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" +\n",
        "              f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "            url = image.getDownloadUrl(\n",
        "                {'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "            r = requests.get(url)\n",
        "            with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                f.write(r.content)\n",
        "            with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                f.extractall(str(path_save))\n",
        "                os.rename(str(path_save/'srtm90_v4.elevation.tif'),\n",
        "                          str(path_save/f'srtm90_v4.elevation_{j}.tif'))\n",
        "                files.append(str(path_save/f'srtm90_v4.elevation_{j}.tif'))\n",
        "            os.remove(str(path_save/'data.zip'))\n",
        "        merge_tifs(files, 'srtm90_v4.elevation.tif', delete=True)\n",
        "\n",
        "def download_data(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" +\n",
        "                       f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                for i in range(1, len(products)):\n",
        "                    imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                            zip_error = True\n",
        "                            for i in range(10): # Try 10 times\n",
        "                                if zip_error:\n",
        "                                    try:\n",
        "                                        url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "                                        r = requests.get(url)\n",
        "                                        with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                                            f.write(r.content)\n",
        "                                        with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                                            files = f.namelist()\n",
        "                                            f.extractall(str(path_save))\n",
        "                                        os.remove(str(path_save/'data.zip'))\n",
        "                                        zip_error = False\n",
        "                                    except:\n",
        "                                        zip_error = True\n",
        "                                        os.remove(str(path_save/'data.zip'))\n",
        "                                        time.sleep(10)\n",
        "                            if zip_error: raise Exception(f'Failed to process {url}')\n",
        "                            for f in files:\n",
        "                                f = path_save/f\n",
        "                                os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "            \n",
        "def download_data_ts(R:RegionST, products, bands, path_save, scale=None, \n",
        "                     download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    for j, R in loop:\n",
        "        region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "        # Merge products to single image collection\n",
        "        imCol = ee.ImageCollection(products[0])\n",
        "        for i in range(1, len(products)):\n",
        "            imCol = imCol.merge(ee.ImageCollection(products[i]))\n",
        "        imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "        imCol = ee.ImageCollection(imCol)\n",
        "        colList = imCol.toList(imCol.size())\n",
        "\n",
        "        # Download each image\n",
        "        for i in range(colList.size().getInfo()):\n",
        "            image = ee.Image(colList.get(i))\n",
        "            zip_error = True\n",
        "            for i in range(10): # Try 10 times\n",
        "                if zip_error:\n",
        "                    try:\n",
        "                        url = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})\n",
        "                        r = requests.get(url)\n",
        "                        with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                            f.write(r.content)\n",
        "                        with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                            files = f.namelist()\n",
        "                            f.extractall(str(path_save))\n",
        "                        os.remove(str(path_save/'data.zip'))\n",
        "                        zip_error = False\n",
        "                    except:\n",
        "                        zip_error = True\n",
        "                        os.remove(str(path_save/'data.zip'))\n",
        "                        time.sleep(10)\n",
        "            if zip_error: raise Exception(f'Failed to process {url}')\n",
        "            for f in files:\n",
        "                f = path_save/f\n",
        "                os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "                \n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "cgDoL5qty-Mg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S2**"
      ],
      "metadata": {
        "id": "qaYr__CZrM7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "    \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "# // Applies scaling factors.\n",
        "def applyScaleFactors_COPERNICUS(image):\n",
        "  opticalBands = image.divide(10000);\n",
        "  return image.addBands(srcImg = opticalBands, overwrite = True)\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_COPERNICUS(R:RegionST, products, bands, path_save, scale=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol).map(applyScaleFactors_COPERNICUS)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_COPERNICUS(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                imCol = ee.ImageCollection(imCol).map(applyScaleFactors_COPERNICUS)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "uz_pX_6Dw0Ji"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-rR0f-GmELJ"
      },
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"COPERNICUS/S2\"]  # Product id in google earth engine\n",
        "bands       = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-04-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_COPERNICUS(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts_COPERNICUS(R, products, bands, path_save, show_progress=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBTGABv5sm0d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from banet.data import open_tif\n",
        "\n",
        "brightness = 3\n",
        "im = np.concatenate([open_tif(f'data/download.{b}.tif').read() for b in bands])\n",
        "im = im.transpose(1,2,0).astype(np.float32)/10000\n",
        "plt.imshow(brightness*im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**GEDI**"
      ],
      "metadata": {
        "id": "Namgq3XVrIgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**gedi code**"
      ],
      "metadata": {
        "id": "FFi8USYVvogC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "def download_data_GEDI(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "JgcwIDqSqzqG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "qDZFInvyrBva"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"LARSE/GEDI/GEDI02_A_002_MONTHLY\"]  # Product id in google earth engine\n",
        "bands       = ['rh98'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-04-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_GEDI(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts_GEDI(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "KOre-TKTrGsW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EO1/HYPERION**"
      ],
      "metadata": {
        "id": "EP8-jdvMdPk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"EO1/HYPERION\"]  # Product id in google earth engine\n",
        "bands       = ['B104', 'B140', 'B221'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2015-03-01', \n",
        "             time_end     = '2016-07-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "#download_data_ts(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "-dgTB5aNdVMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR**"
      ],
      "metadata": {
        "id": "5uoTb-cE4WY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "xSM_t-7M1aOR"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_ALOS(R:RegionST, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_ALOS(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None,\n",
        "                  use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "Yr22qb0kBJPl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR\"]  # Product id in google earth engine\n",
        "bands       = ['HH', 'HV'] # Red, Green, Blue\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-02-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_ALOS(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "# download_data_ts_ALOS(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "3-giNuSezZxi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LANDSAT/LC08/C02/T2_L2**"
      ],
      "metadata": {
        "id": "DsgIikSUjWGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "    \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "# // Applies scaling factors.\n",
        "def applyScaleFactors_LANDSAT(image):\n",
        "  opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n",
        "  thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n",
        "  return image.addBands(srcImg = opticalBands, overwrite = True)\\\n",
        "              .addBands(srcImg = thermalBands, overwrite = True)\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_LANDSAT(R:RegionST, products, bands, path_save, scale=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol).map(applyScaleFactors_LANDSAT)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_LANDSAT(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=500, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "zPbn_-Fgjxin"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "# products    = [\"LANDSAT/LC09/C02/T2_L2\"]  # Product id in google earth engine\n",
        "products    = [\"LANDSAT/LC08/C02/T2_L2\"]  # Product id in google earth engine\n",
        "bands       = ['SR_B1', 'SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7','ST_B10'] # Red, Green, Blue\n",
        "\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 10, \n",
        "             time_start   = '2020-04-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "# download_data_LANDSAT(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "download_data_ts_LANDSAT(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "AfcCwQBujYE8"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S5P/OFFL/L3_CLOUD**"
      ],
      "metadata": {
        "id": "ks5FFYsgcK-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code**"
      ],
      "metadata": {
        "id": "5KdDjQLdcxQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "    \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        # output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_L3_CLOUD(R:RegionST, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      \n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          print(\"collecting 🔻:\",j)\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "          output.clear()\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_L3_CLOUD(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    if not ((path_save/f'download.{bands[0]}.tif').is_file() and \n",
        "           (path_save/f'download.{bands[1]}.tif').is_file() and\n",
        "           (path_save/f'download.{bands[2]}.tif').is_file()):\n",
        "        sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "        fsaves = []\n",
        "        #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "        loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "        url_list.clear()\n",
        "        for j, R in loop:\n",
        "            region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "            if not ((path_save/f'download.{bands[0]}_{j}.tif').is_file() and \n",
        "                   (path_save/f'download.{bands[1]}_{j}.tif').is_file() and\n",
        "                   (path_save/f'download.{bands[2]}_{j}.tif').is_file()):\n",
        "                # Merge products to single image collection\n",
        "                imCol = ee.ImageCollection(products[0])\n",
        "                imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "                if max_cloud_fraction is not None:\n",
        "                    imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "                if use_least_cloudy is not None:\n",
        "                    imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "                im = imCol.median()\n",
        "                imCol = ee.ImageCollection([im])\n",
        "                colList = imCol.toList(imCol.size())\n",
        "                # info = colList.getInfo()\n",
        "                # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "                # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "                # Download each image\n",
        "                for i in range(colList.size().getInfo()):\n",
        "                    image = ee.Image(colList.get(i))\n",
        "                    fname = 'download'\n",
        "                    #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "                    fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "                    fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "                    fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "                    fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "                    if not fnames_full:\n",
        "                        fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                        if not fnames_partial:\n",
        "                          print(\"collecting 🔻:\",j)\n",
        "                          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "                          output.clear()\n",
        "\n",
        "        #download function in parallel fashion from collected urls\n",
        "        download_parallel(url_list) \n",
        "\n",
        "        # Merge files\n",
        "        suffix = '.tif'\n",
        "        files = path_save.ls(include=[suffix])\n",
        "        #files = np.unique(fsaves) \n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        ids = np.unique([int(o.split('_')[-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 6])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            if len(fs) < 500:\n",
        "                fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "                merge_tifs(fs, fsave, delete=True)\n",
        "            else:\n",
        "                fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "                if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                    fs_break.append(fs[(len(fs)//500)*500:])\n",
        "                for fsi, fs2 in enumerate(fs_break):\n",
        "                    fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                    merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "        files = path_save.ls(include=[suffix, '_break'])\n",
        "        files = [o.stem for o in files]\n",
        "        ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        ids = np.unique([o.split('_')[-1]\n",
        "                         for o in files if len(o.split('_')[-1]) < 11])\n",
        "        #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "        file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                    if f'{r}_{i}' in files] for r in ref] \n",
        "        for fs in file_groups:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)"
      ],
      "metadata": {
        "id": "Kb2D2Yvvc0-q"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/data"
      ],
      "metadata": {
        "id": "BWl8x-Ynk7VU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "path_save   = Path('data')\n",
        "# products    = [\"LANDSAT/LC09/C02/T2_L2\"]  # Product id in google earth engine\n",
        "products    = [\"COPERNICUS/S5P/OFFL/L3_CLOUD\"]  # Product id in google earth engine\n",
        "bands       = ['cloud_top_height', 'cloud_base_height'] # Red, Green, Blue\n",
        "\n",
        "\n",
        "###Dataset Availability    2018-07-04T11:34:21Z–2023-01-25T09:44:34\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = 1113.2, \n",
        "             time_start   = '2021-05-10', \n",
        "             time_end     = '2021-06-20')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "# download_data_L3_CLOUD(R, time_window, products, bands, path_save, use_least_cloudy=3, show_progress=True)\n",
        "\n",
        "download_data_ts_L3_CLOUD(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J1BplFXZcMuo",
        "outputId": "24a64190-4367-4a2e-985f-67dd5b09ce4e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [1/1 06:16&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading 🔻: 0 / 579\n",
            " None\n",
            "downloading 🔻: 1 / 579\n",
            " None\n",
            "downloading 🔻: 2 / 579\n",
            " None\n",
            "downloading 🔻: 3 / 579\n",
            " None\n",
            "downloading 🔻: 4 / 579\n",
            " None\n",
            "downloading 🔻: 5 / 579\n",
            " None\n",
            "downloading 🔻: 6 / 579\n",
            " None\n",
            "downloading 🔻: 7 / 579\n",
            " None\n",
            "downloading 🔻: 8 / 579\n",
            " None\n",
            "downloading 🔻: 9 / 579\n",
            " None\n",
            "downloading 🔻: 10 / 579\n",
            " None\n",
            "downloading 🔻: 11 / 579\n",
            " None\n",
            "downloading 🔻: 12 / 579\n",
            " None\n",
            "downloading 🔻: 13 / 579\n",
            " None\n",
            "downloading 🔻: 14 / 579\n",
            " None\n",
            "downloading 🔻: 15 / 579\n",
            " None\n",
            "downloading 🔻: 16 / 579\n",
            " None\n",
            "downloading 🔻: 17 / 579\n",
            " None\n",
            "downloading 🔻: 18 / 579\n",
            " None\n",
            "downloading 🔻: 19 / 579\n",
            " None\n",
            "downloading 🔻: 20 / 579\n",
            " None\n",
            "downloading 🔻: 21 / 579\n",
            " None\n",
            "downloading 🔻: 22 / 579\n",
            " None\n",
            "downloading 🔻: 23 / 579\n",
            " None\n",
            "downloading 🔻: 24 / 579\n",
            " None\n",
            "downloading 🔻: 25 / 579\n",
            " None\n",
            "downloading 🔻: 26 / 579\n",
            " None\n",
            "downloading 🔻: 27 / 579\n",
            " None\n",
            "downloading 🔻: 28 / 579\n",
            " None\n",
            "downloading 🔻: 29 / 579\n",
            " None\n",
            "downloading 🔻: 30 / 579\n",
            " None\n",
            "downloading 🔻: 31 / 579\n",
            " None\n",
            "downloading 🔻: 32 / 579\n",
            " None\n",
            "downloading 🔻: 33 / 579\n",
            " None\n",
            "downloading 🔻: 34 / 579\n",
            " None\n",
            "downloading 🔻: 35 / 579\n",
            " None\n",
            "downloading 🔻: 36 / 579\n",
            " None\n",
            "downloading 🔻: 37 / 579\n",
            " None\n",
            "downloading 🔻: 38 / 579\n",
            " None\n",
            "downloading 🔻: 39 / 579\n",
            " None\n",
            "downloading 🔻: 40 / 579\n",
            " None\n",
            "downloading 🔻: 41 / 579\n",
            " None\n",
            "downloading 🔻: 42 / 579\n",
            " None\n",
            "downloading 🔻: 43 / 579\n",
            " None\n",
            "downloading 🔻: 44 / 579\n",
            " None\n",
            "downloading 🔻: 45 / 579\n",
            " None\n",
            "downloading 🔻: 46 / 579\n",
            " None\n",
            "downloading 🔻: 47 / 579\n",
            " None\n",
            "downloading 🔻: 48 / 579\n",
            " None\n",
            "downloading 🔻: 49 / 579\n",
            " None\n",
            "downloading 🔻: 50 / 579\n",
            " None\n",
            "downloading 🔻: 51 / 579\n",
            " None\n",
            "downloading 🔻: 52 / 579\n",
            " None\n",
            "downloading 🔻: 53 / 579\n",
            " None\n",
            "downloading 🔻: 54 / 579\n",
            " None\n",
            "downloading 🔻: 55 / 579\n",
            " None\n",
            "downloading 🔻: 56 / 579\n",
            " None\n",
            "downloading 🔻: 57 / 579\n",
            " None\n",
            "downloading 🔻: 58 / 579\n",
            " None\n",
            "downloading 🔻: 59 / 579\n",
            " None\n",
            "downloading 🔻: 60 / 579\n",
            " None\n",
            "downloading 🔻: 61 / 579\n",
            " None\n",
            "downloading 🔻: 62 / 579\n",
            " None\n",
            "downloading 🔻: 63 / 579\n",
            " None\n",
            "downloading 🔻: 64 / 579\n",
            " None\n",
            "downloading 🔻: 65 / 579\n",
            " None\n",
            "downloading 🔻: 66 / 579\n",
            " None\n",
            "downloading 🔻: 67 / 579\n",
            " None\n",
            "downloading 🔻: 68 / 579\n",
            " None\n",
            "downloading 🔻: 69 / 579\n",
            " None\n",
            "downloading 🔻: 70 / 579\n",
            " None\n",
            "downloading 🔻: 71 / 579\n",
            " None\n",
            "downloading 🔻: 72 / 579\n",
            " None\n",
            "downloading 🔻: 73 / 579\n",
            " None\n",
            "downloading 🔻: 74 / 579\n",
            " None\n",
            "downloading 🔻: 75 / 579\n",
            " None\n",
            "downloading 🔻: 76 / 579\n",
            " None\n",
            "downloading 🔻: 77 / 579\n",
            " None\n",
            "downloading 🔻: 78 / 579\n",
            " None\n",
            "downloading 🔻: 79 / 579\n",
            " None\n",
            "downloading 🔻: 80 / 579\n",
            " None\n",
            "downloading 🔻: 81 / 579\n",
            " None\n",
            "downloading 🔻: 82 / 579\n",
            " None\n",
            "downloading 🔻: 83 / 579\n",
            " None\n",
            "downloading 🔻: 84 / 579\n",
            " None\n",
            "downloading 🔻: 85 / 579\n",
            " None\n",
            "downloading 🔻: 86 / 579\n",
            " None\n",
            "downloading 🔻: 87 / 579\n",
            " None\n",
            "downloading 🔻: 88 / 579\n",
            " None\n",
            "downloading 🔻: 89 / 579\n",
            " None\n",
            "downloading 🔻: 90 / 579\n",
            " None\n",
            "downloading 🔻: 91 / 579\n",
            " None\n",
            "downloading 🔻: 92 / 579\n",
            " None\n",
            "downloading 🔻: 93 / 579\n",
            " None\n",
            "downloading 🔻: 94 / 579\n",
            " None\n",
            "downloading 🔻: 95 / 579\n",
            " None\n",
            "downloading 🔻: 96 / 579\n",
            " None\n",
            "downloading 🔻: 97 / 579\n",
            " None\n",
            "downloading 🔻: 98 / 579\n",
            " None\n",
            "downloading 🔻: 99 / 579\n",
            " None\n",
            "downloading 🔻: 100 / 579\n",
            " None\n",
            "downloading 🔻: 101 / 579\n",
            " None\n",
            "downloading 🔻: 102 / 579\n",
            " None\n",
            "downloading 🔻: 103 / 579\n",
            " None\n",
            "downloading 🔻: 104 / 579\n",
            " None\n",
            "downloading 🔻: 105 / 579\n",
            " None\n",
            "downloading 🔻: 106 / 579\n",
            " None\n",
            "downloading 🔻: 107 / 579\n",
            " None\n",
            "downloading 🔻: 108 / 579\n",
            " None\n",
            "downloading 🔻: 109 / 579\n",
            " None\n",
            "downloading 🔻: 110 / 579\n",
            " None\n",
            "downloading 🔻: 111 / 579\n",
            " None\n",
            "downloading 🔻: 112 / 579\n",
            " None\n",
            "downloading 🔻: 113 / 579\n",
            " None\n",
            "downloading 🔻: 114 / 579\n",
            " None\n",
            "downloading 🔻: 115 / 579\n",
            " None\n",
            "downloading 🔻: 116 / 579\n",
            " None\n",
            "downloading 🔻: 117 / 579\n",
            " None\n",
            "downloading 🔻: 118 / 579\n",
            " None\n",
            "downloading 🔻: 119 / 579\n",
            " None\n",
            "downloading 🔻: 120 / 579\n",
            " None\n",
            "downloading 🔻: 121 / 579\n",
            " None\n",
            "downloading 🔻: 122 / 579\n",
            " None\n",
            "downloading 🔻: 123 / 579\n",
            " None\n",
            "downloading 🔻: 124 / 579\n",
            " None\n",
            "downloading 🔻: 125 / 579\n",
            " None\n",
            "downloading 🔻: 126 / 579\n",
            " None\n",
            "downloading 🔻: 127 / 579\n",
            " None\n",
            "downloading 🔻: 128 / 579\n",
            " None\n",
            "downloading 🔻: 129 / 579\n",
            " None\n",
            "downloading 🔻: 130 / 579\n",
            " None\n",
            "downloading 🔻: 131 / 579\n",
            " None\n",
            "downloading 🔻: 132 / 579\n",
            " None\n",
            "downloading 🔻: 133 / 579\n",
            " None\n",
            "downloading 🔻: 134 / 579\n",
            " None\n",
            "downloading 🔻: 135 / 579\n",
            " None\n",
            "downloading 🔻: 136 / 579\n",
            " None\n",
            "downloading 🔻: 137 / 579\n",
            " None\n",
            "downloading 🔻: 138 / 579\n",
            " None\n",
            "downloading 🔻: 139 / 579\n",
            " None\n",
            "downloading 🔻: 140 / 579\n",
            " None\n",
            "downloading 🔻: 141 / 579\n",
            " None\n",
            "downloading 🔻: 142 / 579\n",
            " None\n",
            "downloading 🔻: 143 / 579\n",
            " None\n",
            "downloading 🔻: 144 / 579\n",
            " None\n",
            "downloading 🔻: 145 / 579\n",
            " None\n",
            "downloading 🔻: 146 / 579\n",
            " None\n",
            "downloading 🔻: 147 / 579\n",
            " None\n",
            "downloading 🔻: 148 / 579\n",
            " None\n",
            "downloading 🔻: 149 / 579\n",
            " None\n",
            "downloading 🔻: 150 / 579\n",
            " None\n",
            "downloading 🔻: 151 / 579\n",
            " None\n",
            "downloading 🔻: 152 / 579\n",
            " None\n",
            "downloading 🔻: 153 / 579\n",
            " None\n",
            "downloading 🔻: 154 / 579\n",
            " None\n",
            "downloading 🔻: 155 / 579\n",
            " None\n",
            "downloading 🔻: 156 / 579\n",
            " None\n",
            "downloading 🔻: 157 / 579\n",
            " None\n",
            "downloading 🔻: 158 / 579\n",
            " None\n",
            "downloading 🔻: 159 / 579\n",
            " None\n",
            "downloading 🔻: 160 / 579\n",
            " None\n",
            "downloading 🔻: 161 / 579\n",
            " None\n",
            "downloading 🔻: 162 / 579\n",
            " None\n",
            "downloading 🔻: 163 / 579\n",
            " None\n",
            "downloading 🔻: 164 / 579\n",
            " None\n",
            "downloading 🔻: 165 / 579\n",
            " None\n",
            "downloading 🔻: 166 / 579\n",
            " None\n",
            "downloading 🔻: 167 / 579\n",
            " None\n",
            "downloading 🔻: 168 / 579\n",
            " None\n",
            "downloading 🔻: 169 / 579\n",
            " None\n",
            "downloading 🔻: 170 / 579\n",
            " None\n",
            "downloading 🔻: 171 / 579\n",
            " None\n",
            "downloading 🔻: 172 / 579\n",
            " None\n",
            "downloading 🔻: 173 / 579\n",
            " None\n",
            "downloading 🔻: 174 / 579\n",
            " None\n",
            "downloading 🔻: 175 / 579\n",
            " None\n",
            "downloading 🔻: 176 / 579\n",
            " None\n",
            "downloading 🔻: 177 / 579\n",
            " None\n",
            "downloading 🔻: 178 / 579\n",
            " None\n",
            "downloading 🔻: 179 / 579\n",
            " None\n",
            "downloading 🔻: 180 / 579\n",
            " None\n",
            "downloading 🔻: 181 / 579\n",
            " None\n",
            "downloading 🔻: 182 / 579\n",
            " None\n",
            "downloading 🔻: 183 / 579\n",
            " None\n",
            "downloading 🔻: 184 / 579\n",
            " None\n",
            "downloading 🔻: 185 / 579\n",
            " None\n",
            "downloading 🔻: 186 / 579\n",
            " None\n",
            "downloading 🔻: 187 / 579\n",
            " None\n",
            "downloading 🔻: 188 / 579\n",
            " None\n",
            "downloading 🔻: 189 / 579\n",
            " None\n",
            "downloading 🔻: 190 / 579\n",
            " None\n",
            "downloading 🔻: 191 / 579\n",
            " None\n",
            "downloading 🔻: 192 / 579\n",
            " None\n",
            "downloading 🔻: 193 / 579\n",
            " None\n",
            "downloading 🔻: 194 / 579\n",
            " None\n",
            "downloading 🔻: 195 / 579\n",
            " None\n",
            "downloading 🔻: 196 / 579\n",
            " None\n",
            "downloading 🔻: 197 / 579\n",
            " None\n",
            "downloading 🔻: 198 / 579\n",
            " None\n",
            "downloading 🔻: 199 / 579\n",
            " None\n",
            "downloading 🔻: 200 / 579\n",
            " None\n",
            "downloading 🔻: 201 / 579\n",
            " None\n",
            "downloading 🔻: 202 / 579\n",
            " None\n",
            "downloading 🔻: 203 / 579\n",
            " None\n",
            "downloading 🔻: 204 / 579\n",
            " None\n",
            "downloading 🔻: 205 / 579\n",
            " None\n",
            "downloading 🔻: 206 / 579\n",
            " None\n",
            "downloading 🔻: 207 / 579\n",
            " None\n",
            "downloading 🔻: 208 / 579\n",
            " None\n",
            "downloading 🔻: 209 / 579\n",
            " None\n",
            "downloading 🔻: 210 / 579\n",
            " None\n",
            "downloading 🔻: 211 / 579\n",
            " None\n",
            "downloading 🔻: 212 / 579\n",
            " None\n",
            "downloading 🔻: 213 / 579\n",
            " None\n",
            "downloading 🔻: 214 / 579\n",
            " None\n",
            "downloading 🔻: 215 / 579\n",
            " None\n",
            "downloading 🔻: 216 / 579\n",
            " None\n",
            "downloading 🔻: 217 / 579\n",
            " None\n",
            "downloading 🔻: 218 / 579\n",
            " None\n",
            "downloading 🔻: 219 / 579\n",
            " None\n",
            "downloading 🔻: 220 / 579\n",
            " None\n",
            "downloading 🔻: 221 / 579\n",
            " None\n",
            "downloading 🔻: 222 / 579\n",
            " None\n",
            "downloading 🔻: 223 / 579\n",
            " None\n",
            "downloading 🔻: 224 / 579\n",
            " None\n",
            "downloading 🔻: 225 / 579\n",
            " None\n",
            "downloading 🔻: 226 / 579\n",
            " None\n",
            "downloading 🔻: 227 / 579\n",
            " None\n",
            "downloading 🔻: 228 / 579\n",
            " None\n",
            "downloading 🔻: 229 / 579\n",
            " None\n",
            "downloading 🔻: 230 / 579\n",
            " None\n",
            "downloading 🔻: 231 / 579\n",
            " None\n",
            "downloading 🔻: 232 / 579\n",
            " None\n",
            "downloading 🔻: 233 / 579\n",
            " None\n",
            "downloading 🔻: 234 / 579\n",
            " None\n",
            "downloading 🔻: 235 / 579\n",
            " None\n",
            "downloading 🔻: 236 / 579\n",
            " None\n",
            "downloading 🔻: 237 / 579\n",
            " None\n",
            "downloading 🔻: 238 / 579\n",
            " None\n",
            "downloading 🔻: 239 / 579\n",
            " None\n",
            "downloading 🔻: 240 / 579\n",
            " None\n",
            "downloading 🔻: 241 / 579\n",
            " None\n",
            "downloading 🔻: 242 / 579\n",
            " None\n",
            "downloading 🔻: 243 / 579\n",
            " None\n",
            "downloading 🔻: 244 / 579\n",
            " None\n",
            "downloading 🔻: 245 / 579\n",
            " None\n",
            "downloading 🔻: 246 / 579\n",
            " None\n",
            "downloading 🔻: 247 / 579\n",
            " None\n",
            "downloading 🔻: 248 / 579\n",
            " None\n",
            "downloading 🔻: 249 / 579\n",
            " None\n",
            "downloading 🔻: 250 / 579\n",
            " None\n",
            "downloading 🔻: 251 / 579\n",
            " None\n",
            "downloading 🔻: 252 / 579\n",
            " None\n",
            "downloading 🔻: 253 / 579\n",
            " None\n",
            "downloading 🔻: 254 / 579\n",
            " None\n",
            "downloading 🔻: 255 / 579\n",
            " None\n",
            "downloading 🔻: 256 / 579\n",
            " None\n",
            "downloading 🔻: 257 / 579\n",
            " None\n",
            "downloading 🔻: 258 / 579\n",
            " None\n",
            "downloading 🔻: 259 / 579\n",
            " None\n",
            "downloading 🔻: 260 / 579\n",
            " None\n",
            "downloading 🔻: 261 / 579\n",
            " None\n",
            "downloading 🔻: 262 / 579\n",
            " None\n",
            "downloading 🔻: 263 / 579\n",
            " None\n",
            "downloading 🔻: 264 / 579\n",
            " None\n",
            "downloading 🔻: 265 / 579\n",
            " None\n",
            "downloading 🔻: 266 / 579\n",
            " None\n",
            "downloading 🔻: 267 / 579\n",
            " None\n",
            "downloading 🔻: 268 / 579\n",
            " None\n",
            "downloading 🔻: 269 / 579\n",
            " None\n",
            "downloading 🔻: 270 / 579\n",
            " None\n",
            "downloading 🔻: 271 / 579\n",
            " None\n",
            "downloading 🔻: 272 / 579\n",
            " None\n",
            "downloading 🔻: 273 / 579\n",
            " None\n",
            "downloading 🔻: 274 / 579\n",
            " None\n",
            "downloading 🔻: 275 / 579\n",
            " None\n",
            "downloading 🔻: 276 / 579\n",
            " None\n",
            "downloading 🔻: 277 / 579\n",
            " None\n",
            "downloading 🔻: 278 / 579\n",
            " None\n",
            "downloading 🔻: 279 / 579\n",
            " None\n",
            "downloading 🔻: 280 / 579\n",
            " None\n",
            "downloading 🔻: 281 / 579\n",
            " None\n",
            "downloading 🔻: 282 / 579\n",
            " None\n",
            "downloading 🔻: 283 / 579\n",
            " None\n",
            "downloading 🔻: 284 / 579\n",
            " None\n",
            "downloading 🔻: 285 / 579\n",
            " None\n",
            "downloading 🔻: 286 / 579\n",
            " None\n",
            "downloading 🔻: 287 / 579\n",
            " None\n",
            "downloading 🔻: 288 / 579\n",
            " None\n",
            "downloading 🔻: 289 / 579\n",
            " None\n",
            "downloading 🔻: 290 / 579\n",
            " None\n",
            "downloading 🔻: 291 / 579\n",
            " None\n",
            "downloading 🔻: 292 / 579\n",
            " None\n",
            "downloading 🔻: 293 / 579\n",
            " None\n",
            "downloading 🔻: 294 / 579\n",
            " None\n",
            "downloading 🔻: 295 / 579\n",
            " None\n",
            "downloading 🔻: 296 / 579\n",
            " None\n",
            "downloading 🔻: 297 / 579\n",
            " None\n",
            "downloading 🔻: 298 / 579\n",
            " None\n",
            "downloading 🔻: 299 / 579\n",
            " None\n",
            "downloading 🔻: 300 / 579\n",
            " None\n",
            "downloading 🔻: 301 / 579\n",
            " None\n",
            "downloading 🔻: 302 / 579\n",
            " None\n",
            "downloading 🔻: 303 / 579\n",
            " None\n",
            "downloading 🔻: 304 / 579\n",
            " None\n",
            "downloading 🔻: 305 / 579\n",
            " None\n",
            "downloading 🔻: 306 / 579\n",
            " None\n",
            "downloading 🔻: 307 / 579\n",
            " None\n",
            "downloading 🔻: 308 / 579\n",
            " None\n",
            "downloading 🔻: 309 / 579\n",
            " None\n",
            "downloading 🔻: 310 / 579\n",
            " None\n",
            "downloading 🔻: 311 / 579\n",
            " None\n",
            "downloading 🔻: 312 / 579\n",
            " None\n",
            "downloading 🔻: 313 / 579\n",
            " None\n",
            "downloading 🔻: 314 / 579\n",
            " None\n",
            "downloading 🔻: 315 / 579\n",
            " None\n",
            "downloading 🔻: 316 / 579\n",
            " None\n",
            "downloading 🔻: 317 / 579\n",
            " None\n",
            "downloading 🔻: 318 / 579\n",
            " None\n",
            "downloading 🔻: 319 / 579\n",
            " None\n",
            "downloading 🔻: 320 / 579\n",
            " None\n",
            "downloading 🔻: 321 / 579\n",
            " None\n",
            "downloading 🔻: 322 / 579\n",
            " None\n",
            "downloading 🔻: 323 / 579\n",
            " None\n",
            "downloading 🔻: 324 / 579\n",
            " None\n",
            "downloading 🔻: 325 / 579\n",
            " None\n",
            "downloading 🔻: 326 / 579\n",
            " None\n",
            "downloading 🔻: 327 / 579\n",
            " None\n",
            "downloading 🔻: 328 / 579\n",
            " None\n",
            "downloading 🔻: 329 / 579\n",
            " None\n",
            "downloading 🔻: 330 / 579\n",
            " None\n",
            "downloading 🔻: 331 / 579\n",
            " None\n",
            "downloading 🔻: 332 / 579\n",
            " None\n",
            "downloading 🔻: 333 / 579\n",
            " None\n",
            "downloading 🔻: 334 / 579\n",
            " None\n",
            "downloading 🔻: 335 / 579\n",
            " None\n",
            "downloading 🔻: 336 / 579\n",
            " None\n",
            "downloading 🔻: 337 / 579\n",
            " None\n",
            "downloading 🔻: 338 / 579\n",
            " None\n",
            "downloading 🔻: 339 / 579\n",
            " None\n",
            "downloading 🔻: 340 / 579\n",
            " None\n",
            "downloading 🔻: 341 / 579\n",
            " None\n",
            "downloading 🔻: 342 / 579\n",
            " None\n",
            "downloading 🔻: 343 / 579\n",
            " None\n",
            "downloading 🔻: 344 / 579\n",
            " None\n",
            "downloading 🔻: 345 / 579\n",
            " None\n",
            "downloading 🔻: 346 / 579\n",
            " None\n",
            "downloading 🔻: 347 / 579\n",
            " None\n",
            "downloading 🔻: 348 / 579\n",
            " None\n",
            "downloading 🔻: 349 / 579\n",
            " None\n",
            "downloading 🔻: 350 / 579\n",
            " None\n",
            "downloading 🔻: 351 / 579\n",
            " None\n",
            "downloading 🔻: 352 / 579\n",
            " None\n",
            "downloading 🔻: 353 / 579\n",
            " None\n",
            "downloading 🔻: 354 / 579\n",
            " None\n",
            "downloading 🔻: 355 / 579\n",
            " None\n",
            "downloading 🔻: 356 / 579\n",
            " None\n",
            "downloading 🔻: 357 / 579\n",
            " None\n",
            "downloading 🔻: 358 / 579\n",
            " None\n",
            "downloading 🔻: 359 / 579\n",
            " None\n",
            "downloading 🔻: 360 / 579\n",
            " None\n",
            "downloading 🔻: 361 / 579\n",
            " None\n",
            "downloading 🔻: 362 / 579\n",
            " None\n",
            "downloading 🔻: 363 / 579\n",
            " None\n",
            "downloading 🔻: 364 / 579\n",
            " None\n",
            "downloading 🔻: 365 / 579\n",
            " None\n",
            "downloading 🔻: 366 / 579\n",
            " None\n",
            "downloading 🔻: 367 / 579\n",
            " None\n",
            "downloading 🔻: 368 / 579\n",
            " None\n",
            "downloading 🔻: 369 / 579\n",
            " None\n",
            "downloading 🔻: 370 / 579\n",
            " None\n",
            "downloading 🔻: 371 / 579\n",
            " None\n",
            "downloading 🔻: 372 / 579\n",
            " None\n",
            "downloading 🔻: 373 / 579\n",
            " None\n",
            "downloading 🔻: 374 / 579\n",
            " None\n",
            "downloading 🔻: 375 / 579\n",
            " None\n",
            "downloading 🔻: 376 / 579\n",
            " None\n",
            "downloading 🔻: 377 / 579\n",
            " None\n",
            "downloading 🔻: 378 / 579\n",
            " None\n",
            "downloading 🔻: 379 / 579\n",
            " None\n",
            "downloading 🔻: 380 / 579\n",
            " None\n",
            "downloading 🔻: 381 / 579\n",
            " None\n",
            "downloading 🔻: 382 / 579\n",
            " None\n",
            "downloading 🔻: 383 / 579\n",
            " None\n",
            "downloading 🔻: 384 / 579\n",
            " None\n",
            "downloading 🔻: 385 / 579\n",
            " None\n",
            "downloading 🔻: 386 / 579\n",
            " None\n",
            "downloading 🔻: 387 / 579\n",
            " None\n",
            "downloading 🔻: 388 / 579\n",
            " None\n",
            "downloading 🔻: 389 / 579\n",
            " None\n",
            "downloading 🔻: 390 / 579\n",
            " None\n",
            "downloading 🔻: 391 / 579\n",
            " None\n",
            "downloading 🔻: 392 / 579\n",
            " None\n",
            "downloading 🔻: 393 / 579\n",
            " None\n",
            "downloading 🔻: 394 / 579\n",
            " None\n",
            "downloading 🔻: 395 / 579\n",
            " None\n",
            "downloading 🔻: 396 / 579\n",
            " None\n",
            "downloading 🔻: 397 / 579\n",
            " None\n",
            "downloading 🔻: 398 / 579\n",
            " None\n",
            "downloading 🔻: 399 / 579\n",
            " None\n",
            "downloading 🔻: 400 / 579\n",
            " None\n",
            "downloading 🔻: 401 / 579\n",
            " None\n",
            "downloading 🔻: 402 / 579\n",
            " None\n",
            "downloading 🔻: 403 / 579\n",
            " None\n",
            "downloading 🔻: 404 / 579\n",
            " None\n",
            "downloading 🔻: 405 / 579\n",
            " None\n",
            "downloading 🔻: 406 / 579\n",
            " None\n",
            "downloading 🔻: 407 / 579\n",
            " None\n",
            "downloading 🔻: 408 / 579\n",
            " None\n",
            "downloading 🔻: 409 / 579\n",
            " None\n",
            "downloading 🔻: 410 / 579\n",
            " None\n",
            "downloading 🔻: 411 / 579\n",
            " None\n",
            "downloading 🔻: 412 / 579\n",
            " None\n",
            "downloading 🔻: 413 / 579\n",
            " None\n",
            "downloading 🔻: 414 / 579\n",
            " None\n",
            "downloading 🔻: 415 / 579\n",
            " None\n",
            "downloading 🔻: 416 / 579\n",
            " None\n",
            "downloading 🔻: 417 / 579\n",
            " None\n",
            "downloading 🔻: 418 / 579\n",
            " None\n",
            "downloading 🔻: 419 / 579\n",
            " None\n",
            "downloading 🔻: 420 / 579\n",
            " None\n",
            "downloading 🔻: 421 / 579\n",
            " None\n",
            "downloading 🔻: 422 / 579\n",
            " None\n",
            "downloading 🔻: 423 / 579\n",
            " None\n",
            "downloading 🔻: 424 / 579\n",
            " None\n",
            "downloading 🔻: 425 / 579\n",
            " None\n",
            "downloading 🔻: 426 / 579\n",
            " None\n",
            "downloading 🔻: 427 / 579\n",
            " None\n",
            "downloading 🔻: 428 / 579\n",
            " None\n",
            "downloading 🔻: 429 / 579\n",
            " None\n",
            "downloading 🔻: 430 / 579\n",
            " None\n",
            "downloading 🔻: 431 / 579\n",
            " None\n",
            "downloading 🔻: 432 / 579\n",
            " None\n",
            "downloading 🔻: 433 / 579\n",
            " None\n",
            "downloading 🔻: 434 / 579\n",
            " None\n",
            "downloading 🔻: 435 / 579\n",
            " None\n",
            "downloading 🔻: 436 / 579\n",
            " None\n",
            "downloading 🔻: 437 / 579\n",
            " None\n",
            "downloading 🔻: 438 / 579\n",
            " None\n",
            "downloading 🔻: 439 / 579\n",
            " None\n",
            "downloading 🔻: 440 / 579\n",
            " None\n",
            "downloading 🔻: 441 / 579\n",
            " None\n",
            "downloading 🔻: 442 / 579\n",
            " None\n",
            "downloading 🔻: 443 / 579\n",
            " None\n",
            "downloading 🔻: 444 / 579\n",
            " None\n",
            "downloading 🔻: 445 / 579\n",
            " None\n",
            "downloading 🔻: 446 / 579\n",
            " None\n",
            "downloading 🔻: 447 / 579\n",
            " None\n",
            "downloading 🔻: 448 / 579\n",
            " None\n",
            "downloading 🔻: 449 / 579\n",
            " None\n",
            "downloading 🔻: 450 / 579\n",
            " None\n",
            "downloading 🔻: 451 / 579\n",
            " None\n",
            "downloading 🔻: 452 / 579\n",
            " None\n",
            "downloading 🔻: 453 / 579\n",
            " None\n",
            "downloading 🔻: 454 / 579\n",
            " None\n",
            "downloading 🔻: 455 / 579\n",
            " None\n",
            "downloading 🔻: 456 / 579\n",
            " None\n",
            "downloading 🔻: 457 / 579\n",
            " None\n",
            "downloading 🔻: 458 / 579\n",
            " None\n",
            "downloading 🔻: 459 / 579\n",
            " None\n",
            "downloading 🔻: 460 / 579\n",
            " None\n",
            "downloading 🔻: 461 / 579\n",
            " None\n",
            "downloading 🔻: 462 / 579\n",
            " None\n",
            "downloading 🔻: 463 / 579\n",
            " None\n",
            "downloading 🔻: 464 / 579\n",
            " None\n",
            "downloading 🔻: 465 / 579\n",
            " None\n",
            "downloading 🔻: 466 / 579\n",
            " None\n",
            "downloading 🔻: 467 / 579\n",
            " None\n",
            "downloading 🔻: 468 / 579\n",
            " None\n",
            "downloading 🔻: 469 / 579\n",
            " None\n",
            "downloading 🔻: 470 / 579\n",
            " None\n",
            "downloading 🔻: 471 / 579\n",
            " None\n",
            "downloading 🔻: 472 / 579\n",
            " None\n",
            "downloading 🔻: 473 / 579\n",
            " None\n",
            "downloading 🔻: 474 / 579\n",
            " None\n",
            "downloading 🔻: 475 / 579\n",
            " None\n",
            "downloading 🔻: 476 / 579\n",
            " None\n",
            "downloading 🔻: 477 / 579\n",
            " None\n",
            "downloading 🔻: 478 / 579\n",
            " None\n",
            "downloading 🔻: 479 / 579\n",
            " None\n",
            "downloading 🔻: 480 / 579\n",
            " None\n",
            "downloading 🔻: 481 / 579\n",
            " None\n",
            "downloading 🔻: 482 / 579\n",
            " None\n",
            "downloading 🔻: 483 / 579\n",
            " None\n",
            "downloading 🔻: 484 / 579\n",
            " None\n",
            "downloading 🔻: 485 / 579\n",
            " None\n",
            "downloading 🔻: 486 / 579\n",
            " None\n",
            "downloading 🔻: 487 / 579\n",
            " None\n",
            "downloading 🔻: 488 / 579\n",
            " None\n",
            "downloading 🔻: 489 / 579\n",
            " None\n",
            "downloading 🔻: 490 / 579\n",
            " None\n",
            "downloading 🔻: 491 / 579\n",
            " None\n",
            "downloading 🔻: 492 / 579\n",
            " None\n",
            "downloading 🔻: 493 / 579\n",
            " None\n",
            "downloading 🔻: 494 / 579\n",
            " None\n",
            "downloading 🔻: 495 / 579\n",
            " None\n",
            "downloading 🔻: 496 / 579\n",
            " None\n",
            "downloading 🔻: 497 / 579\n",
            " None\n",
            "downloading 🔻: 498 / 579\n",
            " None\n",
            "downloading 🔻: 499 / 579\n",
            " None\n",
            "downloading 🔻: 500 / 579\n",
            " None\n",
            "downloading 🔻: 501 / 579\n",
            " None\n",
            "downloading 🔻: 502 / 579\n",
            " None\n",
            "downloading 🔻: 503 / 579\n",
            " None\n",
            "downloading 🔻: 504 / 579\n",
            " None\n",
            "downloading 🔻: 505 / 579\n",
            " None\n",
            "downloading 🔻: 506 / 579\n",
            " None\n",
            "downloading 🔻: 507 / 579\n",
            " None\n",
            "downloading 🔻: 508 / 579\n",
            " None\n",
            "downloading 🔻: 509 / 579\n",
            " None\n",
            "downloading 🔻: 510 / 579\n",
            " None\n",
            "downloading 🔻: 511 / 579\n",
            " None\n",
            "downloading 🔻: 512 / 579\n",
            " None\n",
            "downloading 🔻: 513 / 579\n",
            " None\n",
            "downloading 🔻: 514 / 579\n",
            " None\n",
            "downloading 🔻: 515 / 579\n",
            " None\n",
            "downloading 🔻: 516 / 579\n",
            " None\n",
            "downloading 🔻: 517 / 579\n",
            " None\n",
            "downloading 🔻: 518 / 579\n",
            " None\n",
            "downloading 🔻: 519 / 579\n",
            " None\n",
            "downloading 🔻: 520 / 579\n",
            " None\n",
            "downloading 🔻: 521 / 579\n",
            " None\n",
            "downloading 🔻: 522 / 579\n",
            " None\n",
            "downloading 🔻: 523 / 579\n",
            " None\n",
            "downloading 🔻: 524 / 579\n",
            " None\n",
            "downloading 🔻: 525 / 579\n",
            " None\n",
            "downloading 🔻: 526 / 579\n",
            " None\n",
            "downloading 🔻: 527 / 579\n",
            " None\n",
            "downloading 🔻: 528 / 579\n",
            " None\n",
            "downloading 🔻: 529 / 579\n",
            " None\n",
            "downloading 🔻: 530 / 579\n",
            " None\n",
            "downloading 🔻: 531 / 579\n",
            " None\n",
            "downloading 🔻: 532 / 579\n",
            " None\n",
            "downloading 🔻: 533 / 579\n",
            " None\n",
            "downloading 🔻: 534 / 579\n",
            " None\n",
            "downloading 🔻: 535 / 579\n",
            " None\n",
            "downloading 🔻: 536 / 579\n",
            " None\n",
            "downloading 🔻: 537 / 579\n",
            " None\n",
            "downloading 🔻: 538 / 579\n",
            " None\n",
            "downloading 🔻: 539 / 579\n",
            " None\n",
            "downloading 🔻: 540 / 579\n",
            " None\n",
            "downloading 🔻: 541 / 579\n",
            " None\n",
            "downloading 🔻: 542 / 579\n",
            " None\n",
            "downloading 🔻: 543 / 579\n",
            " None\n",
            "downloading 🔻: 544 / 579\n",
            " None\n",
            "downloading 🔻: 545 / 579\n",
            " None\n",
            "downloading 🔻: 546 / 579\n",
            " None\n",
            "downloading 🔻: 547 / 579\n",
            " None\n",
            "downloading 🔻: 548 / 579\n",
            " None\n",
            "downloading 🔻: 549 / 579\n",
            " None\n",
            "downloading 🔻: 550 / 579\n",
            " None\n",
            "downloading 🔻: 551 / 579\n",
            " None\n",
            "downloading 🔻: 552 / 579\n",
            " None\n",
            "downloading 🔻: 553 / 579\n",
            " None\n",
            "downloading 🔻: 554 / 579\n",
            " None\n",
            "downloading 🔻: 555 / 579\n",
            " None\n",
            "downloading 🔻: 556 / 579\n",
            " None\n",
            "downloading 🔻: 557 / 579\n",
            " None\n",
            "downloading 🔻: 558 / 579\n",
            " None\n",
            "downloading 🔻: 559 / 579\n",
            " None\n",
            "downloading 🔻: 560 / 579\n",
            " None\n",
            "downloading 🔻: 561 / 579\n",
            " None\n",
            "downloading 🔻: 562 / 579\n",
            " None\n",
            "downloading 🔻: 563 / 579\n",
            " None\n",
            "downloading 🔻: 564 / 579\n",
            " None\n",
            "downloading 🔻: 565 / 579\n",
            " None\n",
            "downloading 🔻: 566 / 579\n",
            " None\n",
            "downloading 🔻: 567 / 579\n",
            " None\n",
            "downloading 🔻: 568 / 579\n",
            " None\n",
            "downloading 🔻: 569 / 579\n",
            " None\n",
            "downloading 🔻: 570 / 579\n",
            " None\n",
            "downloading 🔻: 571 / 579\n",
            " None\n",
            "downloading 🔻: 572 / 579\n",
            " None\n",
            "downloading 🔻: 573 / 579\n",
            " None\n",
            "downloading 🔻: 574 / 579\n",
            " None\n",
            "downloading 🔻: 575 / 579\n",
            " None\n",
            "downloading 🔻: 576 / 579\n",
            " None\n",
            "downloading 🔻: 577 / 579\n",
            " None\n",
            "downloading 🔻: 578 / 579\n",
            " None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COPERNICUS/S1_GRD**"
      ],
      "metadata": {
        "id": "P7O0vJ9p-2H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  r = requests.get(url)\n",
        "  with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "      f.write(r.content)\n",
        "  with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "      files = f.namelist()\n",
        "      f.extractall(str(path_save))\n",
        "  os.remove(str(path_save/'data.zip'))\n",
        "  print(\"downloading 🔻:\",url_list.index(url),\"/\",len(url_list))\n",
        "  for f in files:\n",
        "      f = path_save/f\n",
        "      os.rename(str(f), str(path_save/f'{f.stem}_{url_list.index(url)}{f.suffix}'))\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def download_data_ts_S1_GRD(R:RegionST, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    times = (R.times[0], R.times[-1])\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "      region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "      # Merge products to single image collection\n",
        "      imCol = ee.ImageCollection(products[0])\n",
        "      imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "      imCol = ee.ImageCollection(imCol)\n",
        "      colList = imCol.toList(imCol.size())\n",
        "  \n",
        "      # Download each image\n",
        "      for i in range(colList.size().getInfo()):\n",
        "          image = ee.Image(colList.get(i))\n",
        "          url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list)   \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 6])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                     for o in files if len(o.split('_')[-1]) < 11])\n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n",
        "\n",
        "\n",
        "def download_data_S1_GRD(R:RegionST, times, products, bands, path_save, scale=None, max_cloud_fraction=None, use_least_cloudy=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    # print(sR)\n",
        "    fsaves = []\n",
        "    #for j, R in tqdm(enumerate(sR), total=len(sR)):\n",
        "    loop = enumerate(sR) if not show_progress else progress_bar(enumerate(sR),total=len(sR))\n",
        "    url_list.clear()\n",
        "    for j, R in loop:\n",
        "        print(R)\n",
        "        region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "\n",
        "        # Merge products to single image collection\n",
        "        imCol = ee.ImageCollection(products[0])\n",
        "        imCol = filter_region(imCol, R, times=times, bands=bands)\n",
        "        if max_cloud_fraction is not None:\n",
        "            imCol = filter_cloudy(imCol, max_cloud_fraction=max_cloud_fraction)\n",
        "        if use_least_cloudy is not None:\n",
        "            imCol = n_least_cloudy(imCol, n=use_least_cloudy)\n",
        "        im = imCol.median()\n",
        "        imCol = ee.ImageCollection([im])\n",
        "        colList = imCol.toList(imCol.size())\n",
        "        # info = colList.getInfo()\n",
        "        # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "        # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "        # Download each image\n",
        "        for i in range(colList.size().getInfo()):\n",
        "            image = ee.Image(colList.get(i))\n",
        "            fname = 'download'\n",
        "            #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "            fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "            fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "            fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "            fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "            if not fnames_full:\n",
        "                fsaves.append([path_save/f for f in fnames_partial0])\n",
        "                if not fnames_partial:\n",
        "                  url_list.append(str(image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326','region': f'{region}'})))\n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "    download_parallel(url_list) \n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    #files = np.unique(fsaves) \n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n"
      ],
      "metadata": {
        "id": "rafg5yLa-5FB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "import ee\n",
        "import os\n",
        "import requests\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "from IPython.core.debugger import set_trace\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from fastprogress.fastprogress import progress_bar\n",
        "from banet.geo import open_tif, merge, Region\n",
        "from banet.geo import downsample\n",
        "   \n",
        "   \n",
        "import requests\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from google.colab import output\n",
        "from FireHR.data import *\n",
        "   \n",
        "cpus = cpu_count()\n",
        "url_list = []\n",
        "\n",
        "###########################################################################################################################\n",
        "def download_url(url):\n",
        "  print(\"Downloading🔻\")\n",
        "  j = next((i for i, item in enumerate(uni) if item == url), -1)\n",
        "  R = url\n",
        "  region = (f\"[[{R.bbox.left}, {R.bbox.bottom}], [{R.bbox.right}, {R.bbox.bottom}], \" + f\"[{R.bbox.right}, {R.bbox.top}], [{R.bbox.left}, {R.bbox.top}]]\")\n",
        "  # Merge products to single image collection\n",
        "  imCol = ee.ImageCollection(products[0])\n",
        "  imCol = filter_region(imCol, R, times=time_window, bands=bands)\n",
        "  im = imCol.median()\n",
        "  imCol = ee.ImageCollection([im])\n",
        "  colList = imCol.toList(imCol.size())\n",
        "  # info = colList.getInfo()\n",
        "  # data_times = [pd.to_datetime(o['properties']['system:time_start'], unit='ms') for o in info]\n",
        "  # data_cloudy = [o['properties']['CLOUDY_PIXEL_PERCENTAGE'] for o in info]\n",
        "  # Download each image\n",
        "  for i in range(colList.size().getInfo()):\n",
        "      image = ee.Image(colList.get(i))\n",
        "      fname = 'download'\n",
        "      #fname = image.get('system:id').getInfo().split('/')[-1]\n",
        "      fnames_full = [f'{fname}.{b}.tif' for b in bands]\n",
        "      fnames_partial0 = [f'{fname}.{b}_{j}.tif' for b in bands]\n",
        "      fnames_full = all([(path_save/f).is_file() for f in fnames_full])\n",
        "      fnames_partial = all([(path_save/f).is_file() for f in fnames_partial0])\n",
        "      if not fnames_full:\n",
        "          fsaves.append([path_save/f for f in fnames_partial0])\n",
        "          if not fnames_partial:\n",
        "              zip_error = True\n",
        "              for i in range(10): # Try 10 times\n",
        "                  if zip_error:\n",
        "                      try:\n",
        "                          url_x = image.getDownloadURL({'scale': scale, 'crs': 'EPSG:4326', 'region': f'{region}'})\n",
        "                          r = requests.get(url_x)\n",
        "                          with open(str(path_save/'data.zip'), 'wb') as f:\n",
        "                              f.write(r.content)\n",
        "                          with zipfile.ZipFile(str(path_save/'data.zip'), 'r') as f:\n",
        "                              files = f.namelist()\n",
        "                              f.extractall(str(path_save))\n",
        "                          os.remove(str(path_save/'data.zip'))\n",
        "                          zip_error = False\n",
        "                      except:\n",
        "                          zip_error = True\n",
        "                          os.remove(str(path_save/'data.zip'))\n",
        "                          time.sleep(2)\n",
        "              if zip_error: raise Exception(f'Failed to process {url}')\n",
        "              for f in files:\n",
        "                  f = path_save/f\n",
        "                  os.rename(str(f), str(path_save/f'{f.stem}_{j}{f.suffix}'))\n",
        "\n",
        "###########################################################################################################################\n",
        "        \n",
        "\n",
        "def download_parallel(args):\n",
        "    cpus = cpu_count()\n",
        "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args)\n",
        "    for result in results:\n",
        "        print('', result)\n",
        "        # output.clear()\n",
        "###########################################################################################################################\n",
        "\n",
        "\n",
        "uni = []\n",
        "fsaves = []\n",
        "def download_data_S1_GRD(R:RegionST, times, products, bands, path_save, scale=None, download_crop_size=1000, show_progress=False):\n",
        "    if scale is None: scale = R.scale_meters\n",
        "    ee.Initialize()\n",
        "    path_save.mkdir(exist_ok=True, parents=True)\n",
        "    sR = [R] if min(R.shape) <= download_crop_size else split_region(R, size=download_crop_size, cls=RegionST)\n",
        "    # print(sR)\n",
        "    \n",
        "    uni.clear()\n",
        "    uni.append(sR)\n",
        "    \n",
        "    download_parallel(sR) \n",
        "\n",
        "        \n",
        "\n",
        "    #download function in parallel fashion from collected urls\n",
        "\n",
        "    # Merge files\n",
        "    suffix = '.tif'\n",
        "    files = path_save.ls(include=[suffix])\n",
        "    #files = np.unique(fsaves) \n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    ids = np.unique([int(o.split('_')[-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 6])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        if len(fs) < 500:\n",
        "            fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "            merge_tifs(fs, fsave, delete=True)\n",
        "        else:\n",
        "            fs_break = np.array(fs)[:(len(fs)//500)*500].reshape(len(fs)//500,-1).tolist()\n",
        "            if len(fs[(len(fs)//500)*500:]) > 0:\n",
        "                fs_break.append(fs[(len(fs)//500)*500:])\n",
        "            for fsi, fs2 in enumerate(fs_break):\n",
        "                fsave = '_'.join(fs2[0].stem.split('_')[:-1]) + f'_break{fsi}' + suffix\n",
        "                merge_tifs(fs2, fsave, delete=True)\n",
        "\n",
        "    files = path_save.ls(include=[suffix, '_break'])\n",
        "    files = [o.stem for o in files]\n",
        "    ref = np.unique(['_'.join(o.split('_')[:-1]) \n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    ids = np.unique([o.split('_')[-1]\n",
        "                      for o in files if len(o.split('_')[-1]) < 11])\n",
        "    #file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids] for r in ref] \n",
        "    file_groups = [[path_save/f'{r}_{i}{suffix}' for i in ids \n",
        "                if f'{r}_{i}' in files] for r in ref] \n",
        "    for fs in file_groups:\n",
        "        fsave = '_'.join(fs[0].stem.split('_')[:-1]) + suffix\n",
        "        merge_tifs(fs, fsave, delete=True)\n"
      ],
      "metadata": {
        "id": "fR_g_Ra9NkAQ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "from FireHR.data import *\n",
        "\n",
        "# Bounding box coordinates\n",
        "left   = 75.979728\n",
        "right  = 77.866667\n",
        "bottom = 31.453599\n",
        "top    = 32.416667\n",
        "\n",
        "\n",
        "path_save   = Path('data')\n",
        "products    = [\"JAXA/ALOS/PALSAR-2/Level2_2/ScanSAR\"]  # Product id in google earth engine\n",
        "bands       = ['HH', 'HV'] # Red, Green, Blue\n",
        "scale       = 10\n",
        "\n",
        "\n",
        "R = RegionST(name         = 'TeslaGigaBerlin', \n",
        "             bbox         = [left,bottom,right,top], \n",
        "             scale_meters = scale, \n",
        "             time_start   = '2020-02-01', \n",
        "             time_end     = '2020-04-25')\n",
        "\n",
        "# Download time series\n",
        "# download_data_ts(R, products, bands, path_save)\n",
        "\n",
        "time_window = R.times[0], R.times[-1]\n",
        "\n",
        "# Download median composite of the 3 least cloudy images within the time_window\n",
        "download_data_S1_GRD(R, time_window, products, bands, path_save, show_progress=True)\n",
        "\n",
        "# download_data_ts_S1_GRD(R, products, bands, path_save, show_progress=True)"
      ],
      "metadata": {
        "id": "dlBRU5LLABBv",
        "outputId": "08dc6eed-248f-4427-ff8b-aa564231bfc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n",
            " None\n"
          ]
        }
      ]
    }
  ]
}